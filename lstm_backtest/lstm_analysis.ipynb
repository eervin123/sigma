{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import vectorbtpro as vbt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vbt.settings.plotting[\"layout\"][\"template\"] = \"vbt_dark\"\n",
    "vbt.settings.plotting[\"layout\"][\"width\"] = 1500\n",
    "vbt.settings.plotting['layout']['height'] = 600\n",
    "vbt.settings.wrapping[\"freq\"] = \"1m\"\n",
    "# vbt.settings.portfolio['size_granularity'] = 0.001\n",
    "vbt.settings.portfolio['init_cash'] = 10000\n",
    "\n",
    "# Wherever you saved the pickle file\n",
    "spot_data_path = '/Users/ericervin/Documents/Coding/data-repository/data/BTCUSDT_1m.pkl'\n",
    "futures_data_path = '/Users/ericervin/Documents/Coding/data-repository/data/BTCUSDT_1m_futures.pkl'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the LSTM recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Turn this into a function\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Import all of the pickle files from lstm_backtest/lstm_entries_1e5b9ae6 folder\n",
    "pickle_files = glob.glob('/Users/ericervin/Documents/Coding/sigma/lstm_backtest/lstm_entries_1e5b9ae6/*.pkl')\n",
    "# print(pickle_files)\n",
    "data = []\n",
    "for pickle_file in pickle_files:\n",
    "    with open(pickle_file, 'rb') as f:\n",
    "        new_file = pd.read_pickle(f)\n",
    "        data.append(new_file)\n",
    "data\n",
    "\n",
    "# Define an empty list to store the concatenated dataframes\n",
    "merged_data = []\n",
    "\n",
    "# Loop over each item in the data list\n",
    "for item in data:\n",
    "    # Create the initial dataframe with 'Time', the prices, and 'recommendations' along with 'prediction_details'\n",
    "    df = pd.DataFrame(item['price_data']['Time'], columns=['Time'])\n",
    "    df[['BTCUSDT_Open', 'BTCUSDT_High', 'BTCUSDT_Low', 'BTCUSDT_Close']] = item['price_data'][['BTCUSDT_Open', 'BTCUSDT_High', 'BTCUSDT_Low', 'BTCUSDT_Close']]\n",
    "    df['recommendations'] = item['recommendations']\n",
    "\n",
    "    # Create the dataframe from 'prediction_details'\n",
    "    test = pd.DataFrame(item['prediction_details'], columns=['long', 'short', 'indx_hi', 'indx_low', 'action'])\n",
    "\n",
    "    # Reset the indexes of both dataframes\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Concatenate the dataframes horizontally\n",
    "    merged_df = pd.concat([df, test], axis=1)\n",
    "\n",
    "    # Append the merged dataframe to the list\n",
    "    merged_data.append(merged_df)\n",
    "\n",
    "# Concatenate the dataframes in merged_data vertically\n",
    "result_df = pd.concat(merged_data, axis=0)\n",
    "\n",
    "# Reset the index of the final concatenated dataframe\n",
    "result_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Sort the dataframe by the 'Time' column in ascending order\n",
    "result_df.sort_values('Time', inplace=True)\n",
    "\n",
    "# Reset the index again after sorting\n",
    "result_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print the final concatenated and sorted dataframe\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the resultant forecast columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Turn this into a function that takes in the dataframe and the number of forward prices to add\n",
    "# The function will need to take in our prediction windows of 8, 18, 38, 75, 150\n",
    "\n",
    "# Add the forward 8 prices to the dataframe\n",
    "result_df['BTCUSDT_Open_1'] = result_df['BTCUSDT_Open'].shift(-1)\n",
    "result_df['BTCUSDT_Open_2'] = result_df['BTCUSDT_Open'].shift(-2)\n",
    "result_df['BTCUSDT_Open_3'] = result_df['BTCUSDT_Open'].shift(-3)\n",
    "result_df['BTCUSDT_Open_4'] = result_df['BTCUSDT_Open'].shift(-4)\n",
    "result_df['BTCUSDT_Open_5'] = result_df['BTCUSDT_Open'].shift(-5)\n",
    "result_df['BTCUSDT_Open_6'] = result_df['BTCUSDT_Open'].shift(-6)\n",
    "result_df['BTCUSDT_Open_7'] = result_df['BTCUSDT_Open'].shift(-7)\n",
    "result_df['BTCUSDT_Open_8'] = result_df['BTCUSDT_Open'].shift(-8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculate the highest high price among the forward prices\n",
    "# TODO: this needs to be modified for different numbers of forward prices based on the forecast period\n",
    "highest_high = result_df[['BTCUSDT_Open', 'BTCUSDT_Open_1', 'BTCUSDT_Open_2', 'BTCUSDT_Open_3', 'BTCUSDT_Open_4', 'BTCUSDT_Open_5', 'BTCUSDT_Open_6', 'BTCUSDT_Open_7', 'BTCUSDT_Open_8']].max(axis=1)\n",
    "print(highest_high.shape)\n",
    "# Calculate the percentage of each forward price relative to the current open price to the highest high\n",
    "ranked_forward_prices = result_df[['BTCUSDT_Open', 'BTCUSDT_Open_1', 'BTCUSDT_Open_2', 'BTCUSDT_Open_3', 'BTCUSDT_Open_4', 'BTCUSDT_Open_5', 'BTCUSDT_Open_6', 'BTCUSDT_Open_7']].div(highest_high, axis=0)\n",
    "print(ranked_forward_prices.shape)\n",
    "# Normalize the ranked forward prices to ensure the sum is equal to 1\n",
    "ranked_forward_prices_normalized = ranked_forward_prices.div(ranked_forward_prices.sum(axis=1), axis=0)\n",
    "print(ranked_forward_prices_normalized.shape)\n",
    "# Convert the normalized ranked forward prices to a numpy array\n",
    "array = ranked_forward_prices_normalized.to_numpy()\n",
    "print(array.shape)\n",
    "print(array)\n",
    "\n",
    "# Add the column as 'fwd_8_actual' in result_df\n",
    "result_df['fwd_8_actual'] = array.tolist() # TODO: this may also need to be modified to handle the different forecast periods\n",
    "\n",
    "# Print the modified result_df\n",
    "result_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for accuracy\n",
    "Below we measure the euclidian distance between the two arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['fwd_8_actual'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "result_df = result_df[:-8].copy() # TODO: this should be modified to handle the different forecast periods\n",
    "\n",
    "# Define a function to calculate Euclidean distance\n",
    "def euclidean_distance(arr1, arr2):\n",
    "    return np.sqrt(np.sum((np.array(arr1) - np.array(arr2)) ** 2))\n",
    "\n",
    "# Apply this function to the 'long' and 'fwd_8_actual' columns\n",
    "result_df['long_distance_to_actual'] = result_df.apply(lambda row: euclidean_distance(row['long'], row['fwd_8_actual']), axis=1)\n",
    "result_df['short_distance_to_actual'] = result_df.apply(lambda row: euclidean_distance(row['short'], row['fwd_8_actual']), axis=1)\n",
    "result_df['long_minus_short'] = result_df.apply(lambda row: euclidean_distance(row['long'], row['short']), axis=1) # this is the difference between the long and short predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.index = result_df['Time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the euclidean distance between the long predictions and the short predictions\n",
    "result_df['long_minus_short'].rolling(500).mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'Time' column in your DataFrame 'result_df' is in datetime format and is sorted in ascending order.\n",
    "# Let's calculate rolling mean, median and standard deviation of 'long_distance_to_actual' over time.\n",
    "\n",
    "# Calculate rolling statistics over a window of, say, 500 observations.\n",
    "window_size = 500\n",
    "result_df['Rolling_Mean_Distance'] = result_df['long_distance_to_actual'].rolling(window=window_size).mean()\n",
    "result_df['Rolling_Median_Distance'] = result_df['long_distance_to_actual'].rolling(window=window_size).median()\n",
    "result_df['Rolling_Std_Dev_Distance'] = result_df['long_distance_to_actual'].rolling(window=window_size).std()\n",
    "# Now calc the rolling statistics for 'short_distance_to_actual'\n",
    "result_df['Rolling_Mean_Short_Distance'] = result_df['short_distance_to_actual'].rolling(window=window_size).mean()\n",
    "result_df['Rolling_Median_Short_Distance'] = result_df['short_distance_to_actual'].rolling(window=window_size).median()\n",
    "result_df['Rolling_Std_Dev_Short_Distance'] = result_df['short_distance_to_actual'].rolling(window=window_size).std()\n",
    "# Now calc the rolling statistics for 'long_minus_short'\n",
    "result_df['Rolling_Mean_Long_Minus_Short'] = result_df['long_minus_short'].rolling(window=window_size).mean()\n",
    "result_df['Rolling_Median_Long_Minus_Short'] = result_df['long_minus_short'].rolling(window=window_size).median()\n",
    "result_df['Rolling_Std_Dev_Long_Minus_Short'] = result_df['long_minus_short'].rolling(window=window_size).std()\n",
    "\n",
    "# Let's plot these rolling statistics over time.\n",
    "result_df[[\n",
    "    'Rolling_Mean_Distance',\n",
    "    # 'Rolling_Median_Distance',\n",
    "    'Rolling_Std_Dev_Distance',\n",
    "    'Rolling_Mean_Short_Distance',\n",
    "    # 'Rolling_Median_Short_Distance',\n",
    "    'Rolling_Std_Dev_Short_Distance',\n",
    "    'Rolling_Mean_Long_Minus_Short',\n",
    "    # 'Rolling_Median_Long_Minus_Short',\n",
    "    'Rolling_Std_Dev_Long_Minus_Short'\n",
    "    ]].vbt.plot(width=800).show_svg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = '2023-01-11'\n",
    "end = '2023-01-13'\n",
    "\n",
    "result_df.loc[start:end][['long_distance_to_actual', 'short_distance_to_actual', 'long_minus_short']].vbt.plot(width=800, height=300).show_svg()\n",
    "result_df.loc[start:end][['Rolling_Mean_Distance', 'Rolling_Mean_Short_Distance', 'Rolling_Mean_Long_Minus_Short']].vbt.plot(width=800, height=300).show_svg()\n",
    "result_df.loc[start:end][['Rolling_Std_Dev_Distance', 'Rolling_Std_Dev_Short_Distance', 'Rolling_Std_Dev_Long_Minus_Short']].vbt.plot(width=800, height=300).show_svg()\n",
    "# Plot the price\n",
    "result_df.loc[start:end][['BTCUSDT_Open']].vbt.plot(width=800, height=300).show_svg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Initialize lists to store MAE and MSE values\n",
    "mae_long_fwd, mse_long_fwd = [], []\n",
    "mae_short_fwd, mse_short_fwd = [], []\n",
    "mae_long_short, mse_long_short = [], []\n",
    "\n",
    "# Iterate over each row\n",
    "for i in range(len(result_df)):\n",
    "    long_preds = result_df['long'].iloc[i]\n",
    "    short_preds = result_df['short'].iloc[i]\n",
    "    actuals = result_df['fwd_8_actual'].iloc[i]\n",
    "\n",
    "    # Calculate and store MAE and MSE values\n",
    "    mae_long_fwd.append(mean_absolute_error(actuals, long_preds))\n",
    "    mse_long_fwd.append(mean_squared_error(actuals, long_preds))\n",
    "\n",
    "    mae_short_fwd.append(mean_absolute_error(actuals, short_preds))\n",
    "    mse_short_fwd.append(mean_squared_error(actuals, short_preds))\n",
    "\n",
    "    mae_long_short.append(mean_absolute_error(long_preds, short_preds))\n",
    "    mse_long_short.append(mean_squared_error(long_preds, short_preds))\n",
    "\n",
    "# Create a DataFrame to store the calculated metrics\n",
    "stats_df = pd.DataFrame({\n",
    "    'MAE_Long_Fwd': mae_long_fwd,\n",
    "    'MSE_Long_Fwd': mse_long_fwd,\n",
    "    'MAE_Short_Fwd': mae_short_fwd,\n",
    "    'MSE_Short_Fwd': mse_short_fwd,\n",
    "    'MAE_Long_Short': mae_long_short,\n",
    "    'MSE_Long_Short': mse_long_short,\n",
    "})\n",
    "\n",
    "# Display descriptive statistics for the calculated metrics\n",
    "print(stats_df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc correlations\n",
    "\n",
    "correlation = result_df['long_distance_to_actual'].corr(result_df['short_distance_to_actual'])\n",
    "\n",
    "print(\"Correlation between Euclidean distance between long array and short array and future actual results: \", correlation)\n",
    "\n",
    "correlation_short = result_df['long_distance_to_actual'].corr(result_df['long_minus_short'])\n",
    "\n",
    "print(\"Correlation between difference in long minus short predictions and future actual results for longs: \", correlation_short)\n",
    "\n",
    "correlation_difference = result_df['short_distance_to_actual'].corr(result_df['long_minus_short'])\n",
    "\n",
    "print(\"Correlation between difference in long minus short predictions and future actual results for shorts: \", correlation_difference)\n",
    "\n",
    "long_correlation_slope = result_df['long_slope'].corr(result_df['actual_slope'])\n",
    "print(long_correlation_slope)\n",
    "short_correlation_slope = result_df['short_slope'].corr(result_df['actual_slope'])\n",
    "print(short_correlation_slope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe to store the independent values of the 'long' and 'short' arrays and the dependent value of the 'fwd_8_actual' array\n",
    "# Unpack each array into its own column and prefix each column with 'long_1, 2, 3, ...8' short_1, 2, 3, ...8 and long_minus_short_1, 2, 3, and 'y_actual_1, 2,3..8' respectively\n",
    "# This is done to ensure that the column names are unique\n",
    "# The 'long_minus_short' array is the difference between the 'long' and 'short' arrays\n",
    "\n",
    "# TODO: this should be done in a loop as a function to accommodate different prediction window sizes\n",
    "full_df = pd.DataFrame()\n",
    "full_df['long_minus_short'] = pd.DataFrame(result_df['long_minus_short'], index=result_df.index)\n",
    "full_df['long_distance_to_actual'] = pd.DataFrame(result_df['long_distance_to_actual'], index=result_df.index)\n",
    "full_df['short_distance_to_actual'] = pd.DataFrame(result_df['short_distance_to_actual'], index=result_df.index)\n",
    "full_df[['long_1', 'long_2', 'long_3', 'long_4', 'long_5', 'long_6', 'long_7', 'long_8']] = pd.DataFrame(result_df['long'].tolist(), index=result_df.index)\n",
    "full_df[['short_1', 'short_2', 'short_3', 'short_4', 'short_5', 'short_6', 'short_7', 'short_8']] = pd.DataFrame(result_df['short'].tolist(), index=result_df.index)\n",
    "full_df[['y_actual_1', 'y_actual_2', 'y_actual_3', 'y_actual_4', 'y_actual_5', 'y_actual_6', 'y_actual_7', 'y_actual_8']] = pd.DataFrame(result_df['fwd_8_actual'].tolist(), index=result_df.index)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For sharing with the analyst team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('backtest_results.csv') # This file is most of what the team will need to perform analysis\n",
    "full_df.to_csv('unpacked_arrays.csv') # This is only if someone wants to get into the weeds or possibly feed into another ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatterplot of the long_minus_short and long_distance_to_actual columns and then do one for short_distance_to_actual and long_minus_short\n",
    "# Add the best fit line and show the rsquared value\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.regplot(x='long_minus_short', y='long_distance_to_actual', data=result_df)\n",
    "sns.regplot(x='long_minus_short', y='short_distance_to_actual', data=result_df)\n",
    "r_2 = result_df['long_minus_short'].corr(result_df['long_distance_to_actual']) ** 2\n",
    "print(\"R-squared value for long_minus_short and long_distance_to_actual: \", r_2)\n",
    "short_r_2 = result_df['long_minus_short'].corr(result_df['short_distance_to_actual']) ** 2\n",
    "print(\"R-squared value for long_minus_short and short_distance_to_actual: \", short_r_2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare how accurate the long array is vs the short array\n",
    "a heavy concentration in the lower left is what we would be hoping for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x='short_distance_to_actual', y='long_distance_to_actual', data=result_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's now add the `long_minus_short` color map to see if there are any clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "long_predictions_vs_actual = result_df['long_distance_to_actual']\n",
    "short_predictions_vs_actual = result_df['short_distance_to_actual']\n",
    "long_minus_short = result_df['long_minus_short']\n",
    "\n",
    "# Calculate the slope and intercept for the line of best fit\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(long_predictions_vs_actual, short_predictions_vs_actual)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(long_predictions_vs_actual, short_predictions_vs_actual, c=long_minus_short, cmap='coolwarm', alpha=0.2)\n",
    "\n",
    "# Plot the line of best fit\n",
    "plt.plot(np.unique(long_predictions_vs_actual), np.poly1d(np.polyfit(long_predictions_vs_actual, short_predictions_vs_actual, 1))(np.unique(long_predictions_vs_actual)))\n",
    "# Add X and Y axes labels\n",
    "plt.xlabel('Long predictions vs actuals')\n",
    "plt.ylabel('Short predictions vs actuals')\n",
    "# Add the formula for the line of best fit\n",
    "plt.text(0.2, 0.2, f'y = {slope:.2f}x + {intercept:.2f}', fontsize=15)\n",
    "\n",
    "# Add the legend\n",
    "cbar = plt.colorbar(label='Long minus short \"Confidence Level\" lower is better')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when the long minus short confidence level is high (lower number) the accuracy improves significantly. (lower numbers) Notice the clustering of blue dots in the lower left quadrant"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the slopes of the arrays\n",
    "I'm thinking the general direction of the arrays is an indication of a buying opportunity or selling opportunity rather than simply looking at the first element. This is just a different twist that we may want to use for signal generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the slope of the weights\n",
    "def calculate_slope(weights):\n",
    "    t = np.arange(len(weights))\n",
    "    slope, intercept = np.polyfit(t, weights, 1)\n",
    "    return slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO make this part of the function\n",
    "result_df['long_slope'] = result_df['long'].apply(calculate_slope)\n",
    "result_df['short_slope'] = result_df['short'].apply(calculate_slope)\n",
    "result_df['actual_slope'] = result_df['fwd_8_actual'].apply(calculate_slope)\n",
    "# format floats to 5 decimal places\n",
    "pd.options.display.float_format = '{:.5f}'.format\n",
    "result_df[['long_slope', 'short_slope', 'actual_slope']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate the periods where the actual slope was in the top 30% of all slopes\n",
    "slope_qtile = 0.99\n",
    "result_df[f'actual_slope_top_{int(slope_qtile*100)}'] = result_df['actual_slope'] > result_df['actual_slope'].quantile(slope_qtile)\n",
    "# Isolate the periods where the long slope was in the top 30% of all slopes\n",
    "result_df[f'long_slope_top_{int(slope_qtile*100)}'] = result_df['long_slope'] > result_df['long_slope'].quantile(slope_qtile)\n",
    "result_df[f'long_slope_bottom_{int(1-slope_qtile*100)}'] = result_df['long_slope'] < result_df['long_slope'].quantile(1-slope_qtile)\n",
    "\n",
    "print(f'The number of periods where the actual slope was in the top {slope_qtile}% of all slopes: ', result_df[f'actual_slope_top_{int(slope_qtile*100)}'].sum())\n",
    "print(f'The number of periods where the long slope was in the top {slope_qtile}% of all slopes: ', result_df[f'long_slope_top_{int(slope_qtile*100)}'].sum())\n",
    "print(f'The average long slope during periods where the actual slope was in the top {slope_qtile}% of all slopes: ', result_df[result_df[f'actual_slope_top_{int(slope_qtile*100)}']]['long_slope'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highlight the periods when the long minus short difference was in the bottom 30% of all differences In theory these are the most highly convicted periods\n",
    "lms_qtile = 0.01\n",
    "result_df[f'long_minus_short_lowest_{int(lms_qtile*100)}'] = result_df['long_minus_short'] < result_df['long_minus_short'].quantile(lms_qtile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a quick backtest where the entries and exits are based on the above conditions\n",
    "# If the actual slope is in the top 30% of all slopes and the long_minus_short is in the bottom 30% of all differences then go long if the long_slope and short_slope are both positive\n",
    "# If the actual slope is in the top 30% of all slopes and the long_minus_short is in the bottom 30% of all differences then go short if the long_slope and short_slope are both negative\n",
    "entries = pd.Series(np.where((result_df[f'long_minus_short_lowest_{int(lms_qtile*100)}'] == True) & (result_df['long_slope'] > 0) & (result_df['short_slope'] > 0), True, False))\n",
    "short_entries = pd.Series(np.where((result_df[f'long_minus_short_lowest_{int(lms_qtile*100)}'] == True) & (result_df['long_slope'] < 0) & (result_df['short_slope'] < 0), True, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of entries and exits\n",
    "print(\"Number of long entries: \", entries.sum())\n",
    "print(\"Number of short entries: \", short_entries.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = vbt.Portfolio.from_signals(\n",
    "    high=result_df['BTCUSDT_High'],\n",
    "    low=result_df['BTCUSDT_Low'],\n",
    "    open=result_df['BTCUSDT_Open'],\n",
    "    close=result_df['BTCUSDT_Close'],\n",
    "    entries=entries, # commented out for a short only backtest\n",
    "    short_entries=short_entries,\n",
    "    td_stop = 8, # Hold on to the position for 8 bars\n",
    "    time_delta_format = 'Rows', # Use the row index to calculate the time delta\n",
    "    # tp_stop = 0.01,\n",
    "    accumulate=False,\n",
    "    # sl_stop = 0.005,\n",
    "    )\n",
    "print(pf.stats())\n",
    "# pf.plot(height=600, width=800).show_svg()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up entries and exits based on the slope\n",
    "entry_slope_threshold = 0.07\n",
    "short_entry_slope_threshold = 0.08\n",
    "entries = pd.Series(np.where((result_df['long_slope'] > entry_slope_threshold), True, False))\n",
    "short_entries = pd.Series(np.where((result_df['short_slope'] < -short_entry_slope_threshold), True, False))\n",
    "prediction_period = 8\n",
    "pf = vbt.Portfolio.from_signals(\n",
    "    high=result_df['BTCUSDT_High'],\n",
    "    low=result_df['BTCUSDT_Low'],\n",
    "    open=result_df['BTCUSDT_Open'],\n",
    "    close=result_df['BTCUSDT_Close'],\n",
    "    entries=entries, # commented out for a short only backtest\n",
    "    # exits = result_df['long_slope']<0.04,\n",
    "    short_entries=short_entries,\n",
    "    td_stop = prediction_period, # Hold on to the position for 8 bars\n",
    "    time_delta_format = 'Rows', # Use the row index to calculate the time delta\n",
    "    # tp_stop = 0.01,\n",
    "    accumulate=False,\n",
    "    # sl_stop = 0.005,\n",
    "    )\n",
    "print(pf.stats())\n",
    "# pf.plot(height=600, width=800).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a filtered dataset\n",
    "pick a threshold that you will only accept recommendations when the long_minus_short is below XX. I'm going to start with 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df[result_df['long_minus_short'] < 0.1][['long_minus_short','long_distance_to_actual','short_distance_to_actual','long_slope', 'short_slope', 'actual_slope']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's see how we would do if we listened to the LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import the objects action types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class ActionType(str, Enum):\n",
    "    OPEN_SHORT = \"open-short\"\n",
    "    OPEN_LONG = \"open-long\"\n",
    "    CLOSE_LONG = \"close-long\"\n",
    "    CLOSE_SHORT = \"close-short\"\n",
    "    LEGACY_OPEN_LONG = \"open_long\"\n",
    "    LEGACY_OPEN_SHORT = \"open_short\"\n",
    "    LEGACY_CLOSE_LONG = \"close_long\"\n",
    "    LEGACY_CLOSE_SHORT = \"close_short\"\n",
    "    NOOP = \"no_op\"\n",
    "\n",
    "\n",
    "def get_close_action(direction: ActionType):\n",
    "    if direction == ActionType.OPEN_LONG:\n",
    "        return ActionType.CLOSE_LONG\n",
    "    elif direction == ActionType.OPEN_SHORT:\n",
    "        return ActionType.CLOSE_SHORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create entries and exits based on the long and short predictions but filtered for low values of long_minus_short\n",
    "lms_filter = 0.1\n",
    "entries_lms         = pd.Series(np.where((result_df['long_minus_short'] < lms_filter) & (result_df['recommendations']==ActionType.OPEN_LONG), True, False))\n",
    "short_entries_lms   = pd.Series(np.where((result_df['long_minus_short'] < lms_filter) & (result_df['recommendations']==ActionType.OPEN_SHORT), True, False))\n",
    "print(entries.sum())\n",
    "print(short_entries.sum())\n",
    "long_slope_filter = 0.02\n",
    "entries_slope         = pd.Series(np.where((result_df['long_slope'] > long_slope_filter) & (result_df['recommendations']==ActionType.OPEN_LONG), True, False))\n",
    "short_entries_slope   = pd.Series(np.where((result_df['short_slope'] < -long_slope_filter) & (result_df['recommendations']==ActionType.OPEN_SHORT), True, False))\n",
    "print(entries_slope.sum())\n",
    "print(short_entries_slope.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pf = vbt.Portfolio.from_signals(\n",
    "    high=result_df['BTCUSDT_High'],\n",
    "    low=result_df['BTCUSDT_Low'],\n",
    "    open=result_df['BTCUSDT_Open'],\n",
    "    close=result_df['BTCUSDT_Close'], \n",
    "    # entries=entries_slope, # comment this out for a short only portfolio simulation\n",
    "    # short_entries=short_entries_slope,\n",
    "    entries=entries_lms, # comment this out for a short only portfolio simulation\n",
    "    short_entries=short_entries_lms,\n",
    "    td_stop = 8,# Note we are only holding for 8 bars\n",
    "    time_delta_format = 'Rows',\n",
    "    # tp_stop = 0.01,\n",
    "    # accumulate=False,\n",
    "    # sl_stop = 0.005,\n",
    "    leverage=2,\n",
    "    )\n",
    "print(pf.stats())\n",
    "pf.plot(height=600, width=800).show_svg()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the signals\n",
    "clean_entries, clean_exits = entries.vbt.signals.clean(short_entries)\n",
    "clean_short_entries, clean_short_exits = short_entries.vbt.signals.clean(entries)\n",
    "print(clean_entries.sum())\n",
    "print(clean_short_entries.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_pf = vbt.Portfolio.from_signals(\n",
    "    high=result_df['BTCUSDT_High'],\n",
    "    low=result_df['BTCUSDT_Low'],\n",
    "    open=result_df['BTCUSDT_Open'],\n",
    "    close=result_df['BTCUSDT_Close'],\n",
    "    entries=clean_entries,\n",
    "    exits=clean_exits,\n",
    "    short_entries=clean_short_entries,\n",
    "    short_exits=clean_short_exits,\n",
    "    # td_stop = 8,\n",
    "    # time_delta_format = 'Rows',\n",
    "    # tp_stop = 0.01,\n",
    "    accumulate=False,\n",
    "    # sl_stop = 0.05,\n",
    "    )\n",
    "print(clean_pf.stats())\n",
    "clean_pf.plot(height=600, width=800).show_svg()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def dollar_bar_func(ohlc_df, dollar_bar_size):\n",
    "    # Calculate dollar value traded for each row\n",
    "    ohlc_df['DollarValue'] = ohlc_df['Close'] * ohlc_df['Volume']\n",
    "    \n",
    "    # Calculate cumulative dollar value\n",
    "    ohlc_df['CumulativeDollarValue'] = ohlc_df['DollarValue'].cumsum()\n",
    "    \n",
    "    # Determine the number of dollar bars\n",
    "    num_bars = int(ohlc_df['CumulativeDollarValue'].iloc[-1] / dollar_bar_size)\n",
    "    \n",
    "    # Generate index positions for dollar bars\n",
    "    bar_indices = [0]\n",
    "    cumulative_value = 0\n",
    "    for i in range(1, len(ohlc_df)):\n",
    "        cumulative_value += ohlc_df['DollarValue'].iloc[i]\n",
    "        if cumulative_value >= dollar_bar_size:\n",
    "            bar_indices.append(i)\n",
    "            cumulative_value = 0\n",
    "    \n",
    "    # Create a new dataframe with dollar bars\n",
    "    dollar_bars = []\n",
    "    for i in range(len(bar_indices) - 1):\n",
    "        start_idx = bar_indices[i]\n",
    "        end_idx = bar_indices[i + 1]\n",
    "        # TODO: allow the original dataframe to be dynamic to retain all of the original columns and data so the user can pass a dictionary for how to aggregate each column\n",
    "        dollar_bar = {\n",
    "            'Open': ohlc_df['Open'].iloc[start_idx],\n",
    "            'High': ohlc_df['High'].iloc[start_idx:end_idx].max(),\n",
    "            'Low': ohlc_df['Low'].iloc[start_idx:end_idx].min(),\n",
    "            'Close': ohlc_df['Close'].iloc[end_idx],\n",
    "            'Volume': ohlc_df['Volume'].iloc[start_idx:end_idx].sum(),\n",
    "            'Quote volume': ohlc_df['Quote volume'].iloc[start_idx:end_idx].sum(),\n",
    "            'Trade count': ohlc_df['Trade count'].iloc[start_idx:end_idx].sum(),\n",
    "            'Taker base volume': ohlc_df['Taker base volume'].iloc[start_idx:end_idx].sum(),\n",
    "            'Taker quote volume': ohlc_df['Taker quote volume'].iloc[start_idx:end_idx].sum()\n",
    "        }\n",
    "        \n",
    "        if isinstance(ohlc_df.index, pd.DatetimeIndex):\n",
    "            dollar_bar['Open Time'] = ohlc_df.index[start_idx]\n",
    "            dollar_bar['Close Time'] = ohlc_df.index[end_idx] - pd.Timedelta(milliseconds=1)\n",
    "        elif 'Open Time' in ohlc_df.columns:\n",
    "            dollar_bar['Open Time'] = ohlc_df['Open Time'].iloc[start_idx]\n",
    "            dollar_bar['Close Time'] = ohlc_df['Open Time'].iloc[end_idx] - pd.Timedelta(milliseconds=1)\n",
    "        \n",
    "        dollar_bars.append(dollar_bar)\n",
    "    \n",
    "    dollar_bars_df = pd.concat([pd.DataFrame([bar]) for bar in dollar_bars], ignore_index=True)\n",
    "    # Set the index to be the Open Time\n",
    "    dollar_bars_df.set_index('Open Time', inplace=True)\n",
    "    return dollar_bars_df\n",
    "\n",
    "# Create a simple function to simplify the number so we can use it in our column names\n",
    "def simplify_number(num):\n",
    "    \"\"\"\n",
    "    Simplifies a large number by converting it to a shorter representation with a suffix (K, M, B).\n",
    "    simplify_number(1000) -> 1K\n",
    "    \"\"\"\n",
    "    suffixes = ['', 'K', 'M', 'B']\n",
    "    suffix_index = 0\n",
    "    while abs(num) >= 1000 and suffix_index < len(suffixes) - 1:\n",
    "        num /= 1000.0\n",
    "        suffix_index += 1\n",
    "    suffix = suffixes[suffix_index] if suffix_index > 0 else ''\n",
    "    simplified_num = f'{int(num)}{suffix}'\n",
    "    return simplified_num\n",
    "\n",
    "def custom_merge_and_fill_dollar_bars(original_df, dollar_bars_df, dollar_bar_size):\n",
    "    \"\"\"\n",
    "    Merges the original dataframe with the dollar bars dataframe and fills the NaN values.\n",
    "    \"\"\"\n",
    "    # Add prefix to column names in dollar bars dataframe\n",
    "    # Reset the index\n",
    "    dollar_bars_df.reset_index(inplace=True) # in case open time is the index, we need to reset it to a column so we can rename it  \n",
    "    dollar_bar_prefix = f'db_{simplify_number(dollar_bar_size)}_' # prefix for dollar bar columns eg 'db_90M_Open Time'\n",
    "    dollar_bars_df_renamed = dollar_bars_df.add_prefix(dollar_bar_prefix)\n",
    "\n",
    "    # Convert 'Open Time' columns to pandas datetime format and set them as index\n",
    "    dollar_bars_df_renamed.index = pd.to_datetime(dollar_bars_df_renamed[dollar_bar_prefix + 'Open Time']) # TODO make this dynamic\n",
    "\n",
    "    # Merge the dataframes on the index\n",
    "    merged_df = original_df.merge(dollar_bars_df_renamed, how='left', left_index=True, right_index=True)\n",
    "\n",
    "    # Set the flag for a new dollar bar with prefix\n",
    "    merged_df[dollar_bar_prefix + 'NewDBFlag'] = ~merged_df[dollar_bar_prefix + 'BTCUSDT_Close'].isna() # TODO: make this dynamic\n",
    "\n",
    "    # Forward fill the NaN values for all columns except the new dollar bar flag\n",
    "    columns_to_ffill = [col for col in merged_df.columns if col != dollar_bar_prefix + 'NewDBFlag']\n",
    "    merged_df[columns_to_ffill] = merged_df[columns_to_ffill].fillna(method='ffill')\n",
    "\n",
    "    # Fill the remaining NaN values in the new dollar bar flag column with False\n",
    "    merged_df[dollar_bar_prefix + 'NewDBFlag'] = merged_df[dollar_bar_prefix + 'NewDBFlag'].fillna(False)\n",
    "    \n",
    "    # Assign the renamed 'Open Time' column back to the dataframe\n",
    "    merged_df[dollar_bar_prefix + 'Open Time'] = merged_df[dollar_bar_prefix + 'Open Time'] # TODO: make this dynamic \n",
    "    return merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_merge_and_fill_dollar_bars(original_df, dollar_bars_df, dollar_bar_size):\n",
    "    \"\"\"\n",
    "    Merges the original dataframe with the dollar bars dataframe and fills the NaN values.\n",
    "    \"\"\"\n",
    "    # Add prefix to column names in dollar bars dataframe\n",
    "    dollar_bar_prefix = f'db_{simplify_number(dollar_bar_size)}_' # prefix for dollar bar columns eg 'db_90M_Open Time'\n",
    "    \n",
    "    # Create a copy of the dollar bars dataframe and reset its index\n",
    "    dollar_bars_df = dollar_bars_df.reset_index().copy()  \n",
    "\n",
    "    # Add prefix to columns\n",
    "    dollar_bars_df_renamed = dollar_bars_df.add_prefix(dollar_bar_prefix)\n",
    "\n",
    "    # Convert 'Open Time' columns to pandas datetime format and set them as index\n",
    "    dollar_bars_df_renamed.index = pd.to_datetime(dollar_bars_df_renamed[dollar_bar_prefix + 'Open Time']) \n",
    "\n",
    "    # Create a new dataframe with the same index as the original dataframe and fill it with the values from the dollar bars dataframe\n",
    "    new_dollar_bars_df = pd.DataFrame(index=original_df.index)\n",
    "    new_dollar_bars_df = new_dollar_bars_df.merge(dollar_bars_df_renamed, how='left', left_index=True, right_index=True)\n",
    "\n",
    "    # Merge the original dataframe with the new dollar bars dataframe\n",
    "    merged_df = original_df.merge(new_dollar_bars_df, how='left', left_index=True, right_index=True)\n",
    "\n",
    "    # Set the flag for a new dollar bar with prefix\n",
    "    merged_df[dollar_bar_prefix + 'NewDBFlag'] = ~merged_df[dollar_bar_prefix + 'BTCUSDT_Close'].isna()\n",
    "\n",
    "    # Forward fill the NaN values for all columns except the new dollar bar flag\n",
    "    columns_to_ffill = [col for col in merged_df.columns if col != dollar_bar_prefix + 'NewDBFlag']\n",
    "    merged_df[columns_to_ffill] = merged_df[columns_to_ffill].fillna(method='ffill')\n",
    "\n",
    "    # Fill the remaining NaN values in the new dollar bar flag column with False\n",
    "    merged_df[dollar_bar_prefix + 'NewDBFlag'] = merged_df[dollar_bar_prefix + 'NewDBFlag'].fillna(False)\n",
    "    \n",
    "    return merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dollar_bar_size = 90_000_000 # 90 million\n",
    "dollar_bars_df = result_df.copy()\n",
    "futures_1m = vbt.BinanceData.load(futures_data_path).get()\n",
    "original_df = futures_1m.copy()\n",
    "\n",
    "dollar_bar_prefix = f'db_{simplify_number(dollar_bar_size)}_' # prefix for dollar bar columns eg 'db_90M_Open Time'\n",
    "# Create a copy of the dollar bars dataframe and reset its index\n",
    "# dollar_bars_df = dollar_bars_df.reset_index().copy()  \n",
    "# Rename the column Time to Open Time\n",
    "dollar_bars_df.rename(columns={'Time': 'Open Time'}, inplace=True)\n",
    "# Add prefix to columns\n",
    "dollar_bars_df_renamed = dollar_bars_df.add_prefix(dollar_bar_prefix)\n",
    "dollar_bars_df_renamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge this with the 1 minute futures data so stop losses and take profits can be calculated more appropriately\n",
    "copy_result_df = result_df.copy()\n",
    "print(copy_result_df.columns)\n",
    "# Rename the index to Open Time\n",
    "copy_result_df.index.rename('Open Time', inplace=True)\n",
    "# copy_result_df.reset_index(inplace=True)\n",
    "print(copy_result_df.columns)\n",
    "copy_result_df.index = pd.to_datetime(copy_result_df.index)\n",
    "futures_1m.index = pd.to_datetime(futures_1m.index)\n",
    "\n",
    "full_1m_df = custom_merge_and_fill_dollar_bars(futures_1m,copy_result_df, dollar_bar_size=90_000_000,)\n",
    "# full_1m_df[['Close', 'db_90M_Open Time', 'db_90M_BTCUSDT_Close']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the first 8 days and remove everything after '2023-03-22' the end of the result_df\n",
    "full_1m_df = full_1m_df.loc['2021-01-09': '2023-03-22']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_1m_df['db_90M_NewDBFlag'].sum() # How many dollar bars were created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the dollar bar close with the 1 minute close to see if they match\n",
    "full_1m_df.loc['2023-01-01':'2023-01-03'][['Close', 'db_90M_BTCUSDT_Close']].vbt.plot(title='BTCUSDT Close vs 90M BTCUSDT Close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
