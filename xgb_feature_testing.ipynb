{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c69786c2-7c5b-4b1e-bd0c-52380c8df261",
   "metadata": {},
   "source": [
    "# XGBoost with Cross Validation and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2751478-95ce-47d2-9e80-4d47ed0c7b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numba import njit\n",
    "import vectorbtpro as vbt\n",
    "vbt.settings.set_theme(\"dark\")\n",
    "vbt.settings.plotting[\"layout\"][\"width\"] = 800\n",
    "vbt.settings.plotting['layout']['height'] = 200\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression, LogisticRegression\n",
    "from sklearn.svm import SVR, SVC\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(random_state=42) # random forest classifier\n",
    "from joblib import dump, load\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "935ca582-ef09-40f9-b6ff-c303c98989b1",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "The class Splitter can also be helpful in cross-validating ML models. In particular, you can casually step upon a class SKLSplitter that acts as a regular cross-validator from scikit-learn by subclassing BaseCrossValidator. We'll demonstrate its usage on a simple classification problem of predicting the best entry and exit timings.\n",
    "\n",
    "Before we start, we need to decide on features and labels that should act as predictor and response variables respectively. Features are usually multi-columnar time-series DataFrames where each row contains multiple data points (one per column) that should predict the same row in labels. Labels are usually a single-columnar time-series Series that should be predicted. Ask yourself the following questions to easily come up with a decision:\n",
    "\n",
    "\"How can the future performance be represented, preferably as a single number? Should it be the price at the next bar, the average price change over the next week, a vector of weights for rebalancing, a boolean containing a signal, or something else?\"\n",
    "\"What kind of data that encompasses the past performance is likely to predict the future performance? Should it be indicators, news sentiment index, past backtesting results, or something else?\"\n",
    "\"Which ML model can handle such a task?\" (remember that most models are limited to just a couple of specific feature and label formats!)\n",
    "For the sake of an example, we'll fit a random forest classifier on all TA-Lib indicators stacked along columns to predict the binary labels generated by the label generator TRENDLB, where 1 means an uptrend and 0 means a downtrend. Sounds like fun ðŸ˜Œ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41119985",
   "metadata": {},
   "source": [
    "Build a pipeline to impute and (standard-)normalize the data, [reduce the dimensionality](https://scikit-learn.org/stable/auto_examples/compose/plot_digits_pipe.html) of the features, as well as fit one of the [linear](https://scikit-learn.org/stable/modules/linear_model.html) models to predict the average price change over the next n bars (i.e., regression task!). Based on each prediction, you can then decide whether a position is worth opening or closing out. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "767f3215",
   "metadata": {},
   "source": [
    "# Helper functions\n",
    "Create dollar bars and add them to the original df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6acc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dollar_bar_func(ohlc_df, dollar_bar_size):\n",
    "    # Calculate dollar value traded for each row\n",
    "    ohlc_df['DollarValue'] = ohlc_df['Close'] * ohlc_df['Volume']\n",
    "    \n",
    "    # Calculate cumulative dollar value\n",
    "    ohlc_df['CumulativeDollarValue'] = ohlc_df['DollarValue'].cumsum()\n",
    "    \n",
    "    # Determine the number of dollar bars\n",
    "    num_bars = int(ohlc_df['CumulativeDollarValue'].iloc[-1] / dollar_bar_size)\n",
    "    \n",
    "    # Generate index positions for dollar bars\n",
    "    bar_indices = [0]\n",
    "    cumulative_value = 0\n",
    "    for i in range(1, len(ohlc_df)):\n",
    "        cumulative_value += ohlc_df['DollarValue'].iloc[i]\n",
    "        if cumulative_value >= dollar_bar_size:\n",
    "            bar_indices.append(i)\n",
    "            cumulative_value = 0\n",
    "    \n",
    "    # Create a new dataframe with dollar bars\n",
    "    dollar_bars = []\n",
    "    for i in range(len(bar_indices) - 1):\n",
    "        start_idx = bar_indices[i]\n",
    "        end_idx = bar_indices[i + 1]\n",
    "        \n",
    "        dollar_bar = {\n",
    "            'Open': ohlc_df['Open'].iloc[start_idx],\n",
    "            'High': ohlc_df['High'].iloc[start_idx:end_idx].max(),\n",
    "            'Low': ohlc_df['Low'].iloc[start_idx:end_idx].min(),\n",
    "            'Close': ohlc_df['Close'].iloc[end_idx-1],\n",
    "            'Volume': ohlc_df['Volume'].iloc[start_idx:end_idx].sum(),\n",
    "            'Quote volume': ohlc_df['Quote volume'].iloc[start_idx:end_idx].sum(),\n",
    "            'Trade count': ohlc_df['Trade count'].iloc[start_idx:end_idx].sum(),\n",
    "            'Taker base volume': ohlc_df['Taker base volume'].iloc[start_idx:end_idx].sum(),\n",
    "            'Taker quote volume': ohlc_df['Taker quote volume'].iloc[start_idx:end_idx].sum()\n",
    "        }\n",
    "        \n",
    "        if isinstance(ohlc_df.index, pd.DatetimeIndex):\n",
    "            dollar_bar['Open Time'] = ohlc_df.index[start_idx]\n",
    "            dollar_bar['Close Time'] = ohlc_df.index[end_idx-1] - pd.Timedelta(milliseconds=1)\n",
    "        elif 'Open Time' in ohlc_df.columns:\n",
    "            dollar_bar['Open Time'] = ohlc_df['Open Time'].iloc[start_idx]\n",
    "            dollar_bar['Close Time'] = ohlc_df['Open Time'].iloc[end_idx] - pd.Timedelta(milliseconds=1)\n",
    "        \n",
    "        dollar_bars.append(dollar_bar)\n",
    "    \n",
    "    dollar_bars_df = pd.concat([pd.DataFrame([bar]) for bar in dollar_bars], ignore_index=True)\n",
    "    \n",
    "    return dollar_bars_df\n",
    "\n",
    "# Create a simple function to simplify the number so we can use it in our column names\n",
    "def simplify_number(num):\n",
    "    \"\"\"\n",
    "    Simplifies a large number by converting it to a shorter representation with a suffix (K, M, B).\n",
    "    simplify_number(1000) -> 1K\n",
    "    \"\"\"\n",
    "    suffixes = ['', 'K', 'M', 'B']\n",
    "    suffix_index = 0\n",
    "\n",
    "    while abs(num) >= 1000 and suffix_index < len(suffixes) - 1:\n",
    "        num /= 1000.0\n",
    "        suffix_index += 1\n",
    "\n",
    "    suffix = suffixes[suffix_index] if suffix_index > 0 else ''\n",
    "    simplified_num = f'{int(num)}{suffix}'\n",
    "\n",
    "    return simplified_num\n",
    "\n",
    "def merge_and_fill_dollar_bars(original_df, dollar_bars_df, dollar_bar_size):\n",
    "    # Add prefix to column names in dollar bars dataframe\n",
    "    dollar_bar_prefix = f'db_{simplify_number(dollar_bar_size)}_'\n",
    "    dollar_bars_df_renamed = dollar_bars_df.add_prefix(dollar_bar_prefix)\n",
    "\n",
    "    # Convert 'Open Time' columns to pandas datetime format and set them as index\n",
    "    dollar_bars_df_renamed.index = pd.to_datetime(dollar_bars_df_renamed[dollar_bar_prefix + 'Open Time'])\n",
    "\n",
    "    # Merge the dataframes on the index\n",
    "    merged_df = original_df.merge(dollar_bars_df_renamed, how='left', left_index=True, right_index=True)\n",
    "\n",
    "    # Set the flag for a new dollar bar with prefix\n",
    "    merged_df[dollar_bar_prefix + 'NewDBFlag'] = ~merged_df[dollar_bar_prefix + 'Close'].isna()\n",
    "\n",
    "    # Forward fill the NaN values for all columns except the new dollar bar flag\n",
    "    columns_to_ffill = [col for col in merged_df.columns if col != dollar_bar_prefix + 'NewDBFlag']\n",
    "    merged_df[columns_to_ffill] = merged_df[columns_to_ffill].fillna(method='ffill')\n",
    "\n",
    "    # Fill the remaining NaN values in the new dollar bar flag column with False\n",
    "    merged_df[dollar_bar_prefix + 'NewDBFlag'] = merged_df[dollar_bar_prefix + 'NewDBFlag'].fillna(False)\n",
    "    \n",
    "    # Assign the renamed 'Open Time' column back to the dataframe\n",
    "    merged_df[dollar_bar_prefix + 'Open Time'] = merged_df[dollar_bar_prefix + 'Open Time']\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "469e8b28",
   "metadata": {},
   "source": [
    "# Calculate Dollar Bars\n",
    "Calc Dollar bars and then add technical analysis features\n",
    "\n",
    "Uncomment this section if you want to run different size dollar bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1cf2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# futures_1m = vbt.BinanceData.load('/Users/ericervin/Documents/Coding/data-repository/data/BTCUSDT_1m_futures.pkl')\n",
    "# futures_1m_df = futures_1m.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c2b8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dollar_bar_size = 90_000_000\n",
    "# btc_dollar_bars = dollar_bar_func(futures_1m_df, dollar_bar_size=dollar_bar_size)\n",
    "# btc_dollar_bars.index = pd.to_datetime(btc_dollar_bars['Open Time'])\n",
    "# btc_dollar_bars.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640f40b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataframe back into a vbt data object\n",
    "# btc_90M_db_vbt = vbt.BinanceData.from_data(btc_dollar_bars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8cee12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dollarbars to a pickle file\n",
    "# btc_90M_db_vbt.save('data/btc_90M_db_vbt.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c770a46d",
   "metadata": {},
   "source": [
    "# Load the dollar bars from pickle file\n",
    "Take a small slice of the data for train/testing and leave some to be out of sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2883b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all the ta features we can use\n",
    "print(dir(vbt.indicators))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca083be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_90M_db_vbt = vbt.BinanceData.load('data/btc_90M_db_vbt.pkl')\n",
    "\n",
    "data = btc_90M_db_vbt['2021-01-01':'2023-01-01']\n",
    "outofsample_data = btc_90M_db_vbt['2023-01-01':'2023-06-03']\n",
    "print(data.shape)\n",
    "print(outofsample_data.shape)\n",
    "# Wherever you saved the pickle file\n",
    "data_path = 'data/BTCUSDT_1m.pkl'\n",
    "# min_data = vbt.BinanceData.load(data_path).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7226a7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vbt.phelp(vbt.SUPERTREND.run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eac7c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb159cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "supertrend = data.run(\"supertrend\").supert\n",
    "supertrend"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1cea4860",
   "metadata": {},
   "source": [
    "# Create the functions for the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed275ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data, pivot_up_th=0.10, pivot_down_th=0.10, periods_future=150, drop_cols=['Open Time', 'Close Time']):\n",
    "    \"\"\"\n",
    "    This function prepares the data for training the model.\n",
    "    \n",
    "    Parameters:\n",
    "    data (DataFrame): The original DataFrame containing the data.\n",
    "    pivot_up_th (float): Threshold for upward trend.\n",
    "    pivot_down_th (float): Threshold for downward trend.\n",
    "    periods_future (int): Number of periods in the future to predict.\n",
    "    drop_cols (list): Columns to be dropped from the original DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "    X (DataFrame): The feature matrix.\n",
    "    y (Series): The target vector.\n",
    "    \"\"\"\n",
    "    pivot_up_th2 = pivot_up_th * 1.5\n",
    "    pivot_down_th2 = pivot_down_th * 1.5\n",
    "    pivot_up_th3 = pivot_up_th * 1.5\n",
    "    pivot_down_th3 = pivot_down_th * 1.5\n",
    "    lookback_window = 14*150 # Number of dollar bars we are predicting into the future times the typical RSI lookback window of 14\n",
    "    \n",
    "    # Generate the features (X) and target (y) dataframes\n",
    "    X = data.get()\n",
    "    # Set up multiple pivot trend labels\n",
    "    pivot_info = data.run(\"pivotinfo\", up_th=pivot_up_th, down_th=pivot_down_th)\n",
    "    binary_pivot_labels = np.where(data.close > pivot_info.conf_value,1,0) # Create binary labels for pivot points\n",
    "    X['trend'] = binary_pivot_labels # add pivot label as a feature\n",
    "    \n",
    "    pivot_info2 = data.run(\"pivotinfo\", up_th=pivot_up_th2, down_th=pivot_down_th2)\n",
    "    binary_pivot_labels2 = np.where(data.close > pivot_info2.conf_value,1,0) # Create binary labels for pivot points\n",
    "    X['trend2'] = binary_pivot_labels2 # add pivot label as a feature\n",
    "    \n",
    "    pivot_info3 = data.run(\"pivotinfo\", up_th=pivot_up_th3, down_th=pivot_down_th3)\n",
    "    binary_pivot_labels3 = np.where(data.close > pivot_info3.conf_value,1,0) # Create binary labels for pivot points\n",
    "    X['trend3'] = binary_pivot_labels3 # add pivot label as a feature\n",
    "    \n",
    "    # Add some TA features\n",
    "    X['supert'] = data.run(\"supertrend\").supert\n",
    "    # X['superd'] = data.run(\"supertrend\").superd\n",
    "    # X['superl'] = data.run(\"supertrend\").superl\n",
    "    # X['supers'] = data.run(\"supertrend\").supers\n",
    "    X['vwap'] = data.run(\"VWAP\").vwap\n",
    "    X['rsi'] = data.run(\"rsi\", window=lookback_window).rsi\n",
    "    X['rsi_overbought'] = pd.Series(np.where(X['rsi'] > 70, 1, 0), index=X.index)\n",
    "    X['rsi_oversold'] = pd.Series(np.where(X['rsi'] < 30, 1, 0), index=X.index)\n",
    "    X['bb_width'] = data.run(\"bbands\", window=lookback_window).bandwidth\n",
    "    X['bb_width_pct'] = data.run(\"bbands\", window=lookback_window).percent_b\n",
    "    X['fast_k'] = data.run(\"stoch\", fast_k_window=lookback_window, slow_k_window=50*14, slow_d_window=50*14).fast_k\n",
    "    X['slow_k'] = data.run(\"stoch\", fast_k_window=lookback_window, slow_k_window=50*14, slow_d_window=50*14).slow_k\n",
    "    X['slow_k_trending_up'] = X['slow_k'] > X['slow_k'].shift(1)\n",
    "    X['slow_k_trending_down'] = X['slow_k'] < X['slow_k'].shift(1)\n",
    "    X['slow_d'] = data.run(\"stoch\", fast_k_window=lookback_window, slow_k_window=50*14, slow_d_window=50*14).slow_d\n",
    "    X['slow_k_over_slow_d'] = X['slow_k'] > X['slow_d']\n",
    "    X['slow_k_under_slow_d'] = X['slow_k'] < X['slow_d']\n",
    "    \n",
    "    \n",
    "    # Add in historical returns\n",
    "    X['pct_change_20'] = data.close.pct_change(20)\n",
    "    X['pct_change_40'] = data.close.pct_change(40)\n",
    "    X['pct_change_60'] = data.close.pct_change(60)\n",
    "    X['pct_change_100'] = data.close.pct_change(100)\n",
    "    X['pct_change_160'] = data.close.pct_change(160)\n",
    "    X['pct_change_260'] = data.close.pct_change(260)\n",
    "    X['pct_change_420'] = data.close.pct_change(420)\n",
    "    \n",
    "    X['mid_range_momentum']= pd.Series(np.where(X['pct_change_100'] > X['pct_change_420'], 1, 0), index=X.index)\n",
    "    X['short_range_momentum']= pd.Series(np.where(X['pct_change_20'] > X['pct_change_40'], 1, 0), index=X.index)\n",
    "    X['short_over_long_momentum'] = pd.Series(np.where(X['pct_change_20'] > X['pct_change_420'], 1, 0), index=X.index)\n",
    "    X['momentum_trending'] = pd.Series(np.where(X['pct_change_20'] > X['pct_change_20'].shift(1), 1, 0), index=X.index)\n",
    "    \n",
    "    # Drop the time columns\n",
    "    X = X.drop(drop_cols, axis=1)\n",
    "    # Add time features\n",
    "    X['dayofmonth']  = X.index.day\n",
    "    X['month']       = X.index.month\n",
    "    X['year']        = X.index.year\n",
    "    X['hour']        = X.index.hour\n",
    "    X['minute']      = X.index.minute\n",
    "    X['dayofweek']   = X.index.dayofweek\n",
    "    # X['dayofyear']   = X.index.dayofyear\n",
    "\n",
    "    # Now we are trying to generate future price predictions so we will set the y labels to the price change n periods in the future\n",
    "    y = (data.close.shift(-periods_future) / data.close - 1).rolling(periods_future).mean() # future price change we use rolling mean to smooth the data\n",
    "\n",
    "    # Preprocessing steps to handle NaNs\n",
    "    X = X.replace([-np.inf, np.inf], np.nan) # replace inf with nan\n",
    "    invalid_column_mask = X.isnull().all(axis=0)\n",
    "    X = X.loc[:, ~invalid_column_mask] # drop invalid columns\n",
    "    invalid_row_mask = X.isnull().any(axis=1) | y.isnull() # drop rows that have nan in any column or in y\n",
    "\n",
    "    # Drop invalid rows in X and y\n",
    "    X = X.loc[~invalid_row_mask]\n",
    "    y = y.loc[~invalid_row_mask]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def create_pipeline(X, model='xgb'):\n",
    "    \"\"\"\n",
    "    Create a scikit-learn pipeline.\n",
    "\n",
    "    Parameters:\n",
    "    model (str): The model to use in the pipeline. Default is 'xgb' (XGBoost).\n",
    "\n",
    "    Returns:\n",
    "    pipeline (Pipeline): The scikit-learn pipeline.\n",
    "    \"\"\"\n",
    "    X_shape = X.shape\n",
    "    # Construct the pipeline\n",
    "    steps = [\n",
    "        ('imputation', SimpleImputer(strategy='mean')),  # Imputation replaces missing values\n",
    "        ('scaler', StandardScaler()),  # StandardScaler normalizes the data\n",
    "        # ('pca', PCA(n_components=15))  # PCA reduces dimensionality\n",
    "    ]\n",
    "\n",
    "    if model == 'xgb':\n",
    "        steps.append(('model', XGBRegressor(objective='reg:squarederror')))  # XGBoost regression is used as the prediction model\n",
    "    elif model == 'ridge':\n",
    "        steps.append(('model', Ridge()))  # Ridge regression\n",
    "    elif model == 'linear':\n",
    "        steps.append(('model', LinearRegression()))  # Linear regression\n",
    "    elif model == 'logistic':\n",
    "        steps.append(('model', LogisticRegression()))  # Logistic regression\n",
    "    elif model == 'lasso':\n",
    "        steps.append(('model', Lasso()))  # Lasso regression\n",
    "    elif model == 'elasticnet':\n",
    "        steps.append(('model', ElasticNet()))  # ElasticNet regression\n",
    "    elif model == 'svr':\n",
    "        steps.append(('model', SVR()))  # Support Vector Regression\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model name. Choose from 'xgb', 'ridge', 'linear', 'logistic', 'lasso', 'elasticnet', 'svr'.\")\n",
    "\n",
    "    pipeline = Pipeline(steps)\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "def create_cv(X, min_length=600, offset=200, split=-200, set_labels=[\"train\", \"test\"]):\n",
    "    \"\"\"\n",
    "    Create a cross-validation splitter.\n",
    "\n",
    "    Parameters:\n",
    "    X (DataFrame): The feature matrix.\n",
    "    min_length (int): The minimum length of a sample for cross-validation.\n",
    "    offset (int): The offset used in cross-validation splitting.\n",
    "    split (int): Index at which to split the data in cross-validation.\n",
    "    set_labels (list): Labels for the train and test sets in cross-validation.\n",
    "\n",
    "    Returns:\n",
    "    cv_splitter (SKLSplitter): The cross-validation splits created from cv.get_splitter(X).\n",
    "    cv (SKLSplitter): The cross-validation object.\n",
    "    \"\"\"\n",
    "\n",
    "    # Cross-validate Creates a cross-validation object with all the indexes for each cv split\n",
    "    cv = vbt.SKLSplitter(\"from_expanding\", min_length=min_length, offset=offset, split=split, set_labels=set_labels)\n",
    "    cv_splitter = cv.get_splitter(X)\n",
    "    \n",
    "    return cv_splitter, cv\n",
    "\n",
    "def create_cv_with_gap(X, min_length=600, test_amount=200, gap = 150, set_labels=[\"train\", \"test\"]):\n",
    "    \"\"\"\n",
    "    Create a cross-validation splitter.\n",
    "\n",
    "    Parameters:\n",
    "    X (DataFrame): The feature matrix.\n",
    "    min_length (int): The minimum length of a sample for cross-validation.\n",
    "    offset (int): The offset used in cross-validation splitting.\n",
    "    split (int): Index at which to split the data in cross-validation.\n",
    "    set_labels (list): Labels for the train and test sets in cross-validation.\n",
    "\n",
    "    Returns:\n",
    "    cv_splitter (SKLSplitter): The cross-validation splits created from cv.get_splitter(X).\n",
    "    cv (SKLSplitter): The cross-validation object.\n",
    "    \"\"\"\n",
    "\n",
    "    # Cross-validate Creates a cross-validation object with all the indexes for each cv split\n",
    "    cv = vbt.SKLSplitter(\"from_expanding\", \n",
    "                         min_length=min_length, \n",
    "                         offset=test_amount, \n",
    "                         split=(1.0, vbt.RelRange(length=gap, is_gap=True), test_amount), \n",
    "                         set_labels=set_labels,\n",
    "                         split_range_kwargs=dict(backwards=True)\n",
    "                         )\n",
    "    cv_splitter = cv.get_splitter(X)\n",
    "    \n",
    "    return cv_splitter, cv\n",
    "\n",
    "def create_rolling_cv(X, length=2000, split=0.90, offset=True, offsetlen=0, set_labels=[\"train\", \"test\"]):\n",
    "    \"\"\"\n",
    "    Create a cross-validation splitter.\n",
    "\n",
    "    Parameters:\n",
    "    X (DataFrame): The feature matrix.\n",
    "    min_length (int): The minimum length of a sample for cross-validation.\n",
    "    split (float): percent of window to split training vs testing.\n",
    "    set_labels (list): Labels for the train and test sets in cross-validation.\n",
    "    offset (bool): Whether to offset the splits, True shifts the window forward by only the test number.\n",
    "\n",
    "    Returns:\n",
    "    cv_splitter (SKLSplitter): The cross-validation splits created from cv.get_splitter(X).\n",
    "    cv (SKLSplitter): The cross-validation object.\n",
    "    \"\"\"\n",
    "    if offset:\n",
    "        offsetlen = 2*(length * split) - length\n",
    "        cv = vbt.SKLSplitter(\"from_rolling\", length=length, split=split, offset=-offsetlen, offset_anchor=\"prev_end\", set_labels=set_labels)\n",
    "        cv_splitter = cv.get_splitter(X) \n",
    "        return cv_splitter, cv\n",
    "    # Cross-validate Creates a cross-validation object with all the indexes for each cv split\n",
    "    else:\n",
    "        cv = vbt.SKLSplitter(\"from_rolling\", length=length, split=split, set_labels=set_labels) # offset=-offsetlen, offset_anchor=\"prev_end\",\n",
    "        cv_splitter = cv.get_splitter(X) \n",
    "        return cv_splitter, cv\n",
    "    \n",
    "\n",
    "def create_rolling_cv_with_gap(X, length=500, split=0.70, gap=150, set_labels=[\"train\", \"test\"]):\n",
    "    \"\"\"\n",
    "    Create a cross-validation splitter.\n",
    "\n",
    "    Parameters:\n",
    "    X (DataFrame): The feature matrix.\n",
    "    length (int): The length of a sample for cross-validation.\n",
    "    split (float): The percent of the sample to use for training.\n",
    "    gap (int): The gap between the training and test sets.\n",
    "    set_labels (list): Labels for the train and test sets in cross-validation.\n",
    "\n",
    "    Returns:\n",
    "    cv_splitter (SKLSplitter): The cross-validation splits created from cv.get_splitter(X).\n",
    "    cv (SKLSplitter): The cross-validation object.\n",
    "    \"\"\"\n",
    "    assert length > gap, \"Length must be greater than gap\"\n",
    "\n",
    "    split_size = int((length - gap) * split) # Total length of the set minus the gap times the split percent is training set\n",
    "    test_size = length - split_size - gap # Total length minus the training set minus the gap is the test set\n",
    "    offset = -(split_size-test_size) # Offset the split by the difference between the training and test set this gets the next test set to start where the last one ended\n",
    "\n",
    "    cv = vbt.SKLSplitter(\"from_rolling\", \n",
    "                        length=length,\n",
    "                        split=(split_size, vbt.RelRange(length=gap, is_gap=True), 1.0),\n",
    "                        offset=offset,\n",
    "                        set_labels=set_labels)\n",
    "    cv_splitter = cv.get_splitter(X)\n",
    "    return cv_splitter, cv\n",
    "    \n",
    "\n",
    "def cross_validate_and_train(pipeline, X, y, cv_splitter, model_name=\"\", verbose_interval=10, n_clusters=6):\n",
    "    # Predictions\n",
    "    X_slices = cv_splitter.take(X)\n",
    "    y_slices = cv_splitter.take(y)\n",
    "    \n",
    "    # Print total number of splits\n",
    "    total_splits = len(X_slices.index.unique(level=\"split\"))\n",
    "    print(f\"Total number of cross-validation splits: {total_splits}\")\n",
    "\n",
    "\n",
    "    test_labels = []\n",
    "    test_preds = []\n",
    "    for split in X_slices.index.unique(level=\"split\"):  \n",
    "        X_train_slice= X_slices[(split, \"train\")]  \n",
    "        y_train_slice= y_slices[(split, \"train\")] \n",
    "        X_test_slice = X_slices[(split, \"test\")]\n",
    "        y_test_slice = y_slices[(split, \"test\")]\n",
    "        \n",
    "        # Fit the pipeline on the training data\n",
    "        pipeline.fit(X_train_slice, y_train_slice)\n",
    "        \n",
    "        # Fit the KMeans clustering algorithm on the training data using only the original columns in X_slices\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "        kmeans.fit(X_train_slice)\n",
    "        \n",
    "        # Get the cluster labels for the training data using the KMeans clustering algorithm fitted on the training data\n",
    "        train_cluster_labels = kmeans.predict(X_train_slice)\n",
    "\n",
    "        # Add the \"cluster\" column to the training data using the cluster labels obtained above\n",
    "        train_data = X_train_slice.copy()\n",
    "        train_data[\"cluster\"] = train_cluster_labels\n",
    "        # Get the cluster labels for the test data using the KMeans clustering algorithm fitted on the training data\n",
    "        test_cluster_labels = kmeans.predict(X_test_slice) #TODO: use the kmeans model fitted on the training data to predict the cluster labels for the test data\n",
    "        # Get the cluster labels and their counts for the test data\n",
    "        test_cluster_counts = Counter(test_cluster_labels)\n",
    "        # Add the \"cluster\" column to the test data using the cluster labels obtained above\n",
    "        test_data = X_test_slice.copy()\n",
    "        test_data[\"cluster\"] = test_cluster_labels\n",
    "        # Fit the pipeline on the training data\n",
    "        pipeline.fit(train_data, y_train_slice)\n",
    "        \n",
    "        # Make predictions on the test data\n",
    "        test_pred = pipeline.predict(test_data)  \n",
    "        test_pred = pd.Series(test_pred, index=y_test_slice.index)\n",
    "        test_labels.append(y_test_slice)\n",
    "        test_preds.append(test_pred)\n",
    "\n",
    "        # Only print the MSE every 'verbose_interval' splits\n",
    "        if split % verbose_interval == 0:\n",
    "            print(f\"{model_name} Split {split} Mean Squared Error: {mean_squared_error(y_test_slice, test_pred)}\")\n",
    "            # Print the cluster labels and their counts\n",
    "            print(f\"Cluster Sizes:\")\n",
    "            for label, count in test_cluster_counts.items():\n",
    "                print(f\"Cluster {label}: {count}\")\n",
    "\n",
    "    # Concatenate the test labels and predictions into a single Series\n",
    "    test_labels = pd.concat(test_labels).rename(\"labels\")  \n",
    "    test_preds = pd.concat(test_preds).rename(\"preds\")\n",
    "    \n",
    "    # Drop Duplicates\n",
    "    test_labels = test_labels[~test_labels.index.duplicated(keep='first')]\n",
    "    test_preds = test_preds[~test_preds.index.duplicated(keep='first')]\n",
    "    \n",
    "    return pipeline, test_labels, test_preds\n",
    "\n",
    "def evaluate_predictions(test_labels, test_preds, model_name=\"\"):\n",
    "    # Show the accuracy of the predictions\n",
    "    mse = mean_squared_error(test_labels, test_preds)\n",
    "    rmse = np.sqrt(mse)  # or use mean_squared_error with squared=False\n",
    "    mae = mean_absolute_error(test_labels, test_preds)\n",
    "    r2 = r2_score(test_labels, test_preds)\n",
    "\n",
    "    print(f\"{model_name} Mean Squared Error (MSE): {mse}\")\n",
    "    print(f\"{model_name} Root Mean Squared Error (RMSE): {rmse}\")\n",
    "    print(f\"{model_name} Mean Absolute Error (MAE): {mae}\")\n",
    "    print(f\"{model_name} R-squared: {r2}\")\n",
    "\n",
    "    return mse, rmse, mae, r2\n",
    "\n",
    "def extract_feature_importance(pipeline, X):\n",
    "    fitted_model = pipeline.named_steps['model']\n",
    "    feature_names = X.columns.tolist()\n",
    "    feature_names.append('cluster')\n",
    "\n",
    "    # Create a DataFrame using a Dictionary\n",
    "    data = {'feature_names': feature_names, 'feature_importance': fitted_model.feature_importances_}\n",
    "    print(len(feature_names))\n",
    "    print(len(fitted_model.feature_importances_))\n",
    "    fi_df = pd.DataFrame(data)\n",
    "\n",
    "    # Sort the DataFrame in order decreasing feature importance\n",
    "    fi_df.sort_values(by=['feature_importance'], ascending=False, inplace=True)\n",
    "\n",
    "    # Define size of bar plot\n",
    "    plt.figure(figsize=(12,8))\n",
    "    # Plot bar chart\n",
    "    plt.barh(fi_df['feature_names'], fi_df['feature_importance'], align='center')\n",
    "    # Add chart labels\n",
    "    plt.xlabel('FEATURE IMPORTANCE')\n",
    "    plt.ylabel('FEATURE NAMES')\n",
    "    plt.show()\n",
    "\n",
    "    # Print feature names and importance\n",
    "    for index, row in fi_df.iterrows():\n",
    "        print(f\"{row['feature_names']}: {row['feature_importance']}\")\n",
    "        \n",
    "def plot_prediction_vs_actual(x, y, title='Test Predictions vs Actual Results'):\n",
    "    \"\"\"\n",
    "    Plots predictions against actual results and calculates the line of best fit.\n",
    "\n",
    "    Parameters:\n",
    "    x (Series): Predictions\n",
    "    y (Series): Actual results\n",
    "    title (str): Title of the plot\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Create condition masks for different data types\n",
    "    tp_condition = (x > 0) & (y > 0)  # True positives condition\n",
    "    tn_condition = (x < 0) & (y < 0)  # True negatives condition\n",
    "    fp_condition = (x > 0) & (y < 0)  # False positives condition\n",
    "    fn_condition = (x < 0) & (y > 0)  # False negatives condition\n",
    "\n",
    "    # Calculate percent in each condition\n",
    "    tp_percent = (tp_condition.sum() / len(x)) * 100\n",
    "    tn_percent = (tn_condition.sum() / len(x)) * 100\n",
    "    fp_percent = (fp_condition.sum() / len(x)) * 100\n",
    "    fn_percent = (fn_condition.sum() / len(x)) * 100\n",
    "\n",
    "    # Create scatter plots for each condition with different colors\n",
    "    plt.scatter(x[tp_condition], y[tp_condition], color='green', alpha=0.2, label='True Positives')\n",
    "    plt.scatter(x[tn_condition], y[tn_condition], color='pink', alpha=0.2, label='True Negatives')\n",
    "    plt.scatter(x[fp_condition], y[fp_condition], color='blue', alpha=0.2, label='False Positives')\n",
    "    plt.scatter(x[fn_condition], y[fn_condition], color='blue', alpha=0.2, label='False Negatives')\n",
    "\n",
    "    # Calculate the line of best fit\n",
    "    coefficients = np.polyfit(x, y, 1)\n",
    "    polynomial = np.poly1d(coefficients)\n",
    "\n",
    "    # Generate y-values based on the polynomial\n",
    "    y_fit = polynomial(x)\n",
    "\n",
    "    # Plot the line of best fit\n",
    "    plt.plot(x, y_fit, color='red', label='Line of Best Fit')\n",
    "\n",
    "    # Add title and labels to the axes\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predictions')\n",
    "    plt.ylabel('Actual Results')\n",
    "\n",
    "    # Add a legend\n",
    "    plt.legend()\n",
    "\n",
    "    # Print the equation of the line\n",
    "    slope, intercept = coefficients\n",
    "    print(f\"The equation of the regression line is: y = {slope:.3f}x + {intercept:.3f}\")\n",
    "\n",
    "    # Alternatively, you can display the equation on the plot\n",
    "    plt.text(0.05, 0.95, f'y = {slope:.3f}x + {intercept:.3f}', transform=plt.gca().transAxes)\n",
    "\n",
    "    # Print the percent in each quadrant\n",
    "    print(f\"\\nPercentage of True Positives: {tp_percent:.2f}%\")\n",
    "    print(f\"Percentage of True Negatives: {tn_percent:.2f}%\")\n",
    "    print(f\"Percentage of False Positives: {fp_percent:.2f}%\")\n",
    "    print(f\"Percentage of False Negatives: {fn_percent:.2f}%\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "# Simulate a portfolio making trades based on predictions\n",
    "def simulate_pf(data, test_preds, open_long_th=0.01, close_long_th=0.0, close_short_th=0.0, open_short_th=-0.01, plot=True):\n",
    "    insample_pf = vbt.Portfolio.from_signals(\n",
    "    data.close[test_preds.index],  # use only the test set\n",
    "    entries         = test_preds > open_long_th,  # long when probability of price increase is greater than 2%\n",
    "    exits           = test_preds < close_long_th,  # long when probability of price increase is greater than 2%\n",
    "    short_entries   = test_preds < open_short_th,  # long when probability of price increase is greater than 2%\n",
    "    short_exits     = test_preds > close_short_th,  # short when probability prediction is less than -5%\n",
    "    # direction=\"both\" # long and short\n",
    ")\n",
    "    print(insample_pf.stats())\n",
    "    if plot==True:\n",
    "        insample_pf.plot().show()\n",
    "    return insample_pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8703400c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Future Prediction Length\n",
    "periods_future = 150\n",
    "pivot_up_th = 0.01\n",
    "pivot_down_th = 0.01\n",
    "\n",
    "# Establish the training and testing sets for cross validation\n",
    "windowsize = 10000\n",
    "split = 0.90\n",
    "gap = periods_future\n",
    "\n",
    "# Prep Data\n",
    "X, y = prepare_data(data, pivot_up_th=pivot_up_th, pivot_down_th=pivot_down_th, periods_future=periods_future) # in-sample\n",
    "# print(X)\n",
    "Xoos, yoos = prepare_data(outofsample_data, pivot_up_th=pivot_up_th, pivot_down_th=pivot_down_th, periods_future=periods_future) # out-of-sample\n",
    "# print(X.columns)\n",
    "# Set up the pipeline and create the cross validation splits\n",
    "pipeline = create_pipeline(X, model='xgb')\n",
    "cv_splitter, cv = create_rolling_cv_with_gap(X, length=windowsize, split=split, gap=periods_future, set_labels=[\"train\", \"test\"])\n",
    "\n",
    "# Train and cross-validate\n",
    "final_pipeline, test_labels, test_preds = cross_validate_and_train(pipeline, X, y, cv_splitter, model_name=\"In-Sample\", verbose_interval=50)\n",
    "\n",
    "# Evaluate\n",
    "mse, rmse, mae, r2 = evaluate_predictions(test_labels, test_preds, model_name=\"In-Sample\")\n",
    "\n",
    "# Check the scatter plot of predictions vs actual results\n",
    "plot_prediction_vs_actual(test_preds, test_labels, title='In-Sample Predictions vs Actual Results')\n",
    "# Check the primary features that impacted the model\n",
    "extract_feature_importance(final_pipeline, X)\n",
    "\n",
    "# Show heatmap of the predictions ontop of a price plot\n",
    "# data.close.vbt.overlay_with_heatmap(test_preds).show_svg()\n",
    "\n",
    "# Simulate a portfolio\n",
    "insample_pf = simulate_pf(data=data, test_preds=test_preds, open_long_th=0.03, close_long_th=0.0, close_short_th=-0.03, open_short_th=-0.01, plot=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f9b82d",
   "metadata": {},
   "source": [
    "Do the exact same thing but do it with expanding cross validation so the training set gets bigger and bigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b421ebe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Future Prediction Length\n",
    "periods_future = 150\n",
    "pivot_up_th = 0.10\n",
    "pivot_down_th = 0.05\n",
    "\n",
    "# Establish the training and testing sets for cross validation\n",
    "windowsize = 10000\n",
    "split = 0.90\n",
    "test_amount = int(split * windowsize)\n",
    "gap = periods_future\n",
    "\n",
    "# Prep Data\n",
    "X, y = prepare_data(data, pivot_up_th=pivot_up_th, pivot_down_th=pivot_down_th, periods_future=periods_future) # in-sample\n",
    "Xoos, yoos = prepare_data(outofsample_data, pivot_up_th=pivot_up_th, pivot_down_th=pivot_down_th, periods_future=periods_future) # out-of-sample\n",
    "\n",
    "# Set up the pipeline and create the cross validation splits\n",
    "pipeline = create_pipeline(X, model='xgb')\n",
    "cv_splitter, cv = create_cv_with_gap(X, min_length=windowsize, test_amount=test_amount, gap=periods_future, set_labels=[\"train\", \"test\"])\n",
    "\n",
    "# Train and cross-validate\n",
    "final_pipeline, test_labels, test_preds = cross_validate_and_train(pipeline, X, y, cv_splitter, model_name=\"In-Sample\", verbose_interval=50)\n",
    "\n",
    "# Evaluate\n",
    "mse, rmse, mae, r2 = evaluate_predictions(test_labels, test_preds, model_name=\"In-Sample\")\n",
    "\n",
    "# Check the scatter plot of predictions vs actual results\n",
    "plot_prediction_vs_actual(test_preds, test_labels, title='In-Sample Predictions vs Actual Results')\n",
    "# Check the primary features that impacted the model\n",
    "extract_feature_importance(final_pipeline, X)\n",
    "\n",
    "# Show heatmap of the predictions ontop of a price plot\n",
    "# data.close.vbt.overlay_with_heatmap(test_preds).show_svg()\n",
    "\n",
    "# Simulate a portfolio\n",
    "insample_pf = simulate_pf(data=data, test_preds=test_preds, open_long_th=0.10, close_long_th=0.0, close_short_th=-0.0, open_short_th=-0.05, plot=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction_vs_actual(x, y, title='Test Predictions vs Actual Results'):\n",
    "    \"\"\"\n",
    "    Plots predictions against actual results and calculates the line of best fit.\n",
    "\n",
    "    Parameters:\n",
    "    x (Series): Predictions\n",
    "    y (Series): Actual results\n",
    "    title (str): Title of the plot\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Create condition masks for different data types\n",
    "    tp_condition = (x > 0) & (y > 0)  # True positives condition\n",
    "    tn_condition = (x < 0) & (y < 0)  # True negatives condition\n",
    "    fp_condition = (x > 0) & (y < 0)  # False positives condition\n",
    "    fn_condition = (x < 0) & (y > 0)  # False negatives condition\n",
    "\n",
    "    # Calculate percent in each condition\n",
    "    tp_percent = (tp_condition.sum() / len(x)) * 100\n",
    "    tn_percent = (tn_condition.sum() / len(x)) * 100\n",
    "    fp_percent = (fp_condition.sum() / len(x)) * 100\n",
    "    fn_percent = (fn_condition.sum() / len(x)) * 100\n",
    "\n",
    "    # Create scatter plots for each condition with different colors\n",
    "    plt.scatter(x[tp_condition], y[tp_condition], color='green', alpha=0.2, label='True Positives')\n",
    "    plt.scatter(x[tn_condition], y[tn_condition], color='pink', alpha=0.2, label='True Negatives')\n",
    "    plt.scatter(x[fp_condition], y[fp_condition], color='blue', alpha=0.2, label='False Positives')\n",
    "    plt.scatter(x[fn_condition], y[fn_condition], color='blue', alpha=0.2, label='False Negatives')\n",
    "\n",
    "    # Calculate the line of best fit\n",
    "    coefficients = np.polyfit(x, y, 1)\n",
    "    polynomial = np.poly1d(coefficients)\n",
    "\n",
    "    # Generate y-values based on the polynomial\n",
    "    y_fit = polynomial(x)\n",
    "\n",
    "    # Plot the line of best fit\n",
    "    plt.plot(x, y_fit, color='red', label='Line of Best Fit')\n",
    "\n",
    "    # Add title and labels to the axes\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predictions')\n",
    "    plt.ylabel('Actual Results')\n",
    "\n",
    "    # Add a legend\n",
    "    plt.legend()\n",
    "\n",
    "    # Print the equation of the line\n",
    "    slope, intercept = coefficients\n",
    "    print(f\"The equation of the regression line is: y = {slope:.3f}x + {intercept:.3f}\")\n",
    "\n",
    "    # Alternatively, you can display the equation on the plot\n",
    "    plt.text(0.05, 0.95, f'y = {slope:.3f}x + {intercept:.3f}', transform=plt.gca().transAxes)\n",
    "\n",
    "    # Print the percent in each quadrant\n",
    "    print(f\"\\nPercentage of True Positives: {tp_percent:.2f}%\")\n",
    "    print(f\"Percentage of True Negatives: {tn_percent:.2f}%\")\n",
    "    print(f\"Percentage of False Positives: {fp_percent:.2f}%\")\n",
    "    print(f\"Percentage of False Negatives: {fn_percent:.2f}%\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba635ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction_vs_actual(test_preds, test_labels, title='In-Sample Predictions vs Actual Results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2334e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_preds)\n",
    "# check for duplicates in index\n",
    "print(test_preds.index.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15e48ec",
   "metadata": {},
   "source": [
    "If you want to look at different portfolio simulations you can change the thresholds for opening and closing trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f5a942",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_param_pf = simulate_pf(\n",
    "    data,\n",
    "    test_preds,\n",
    "    open_long_th    =0.10,\n",
    "    close_long_th   =0.0,\n",
    "    close_short_th  =-0.05,\n",
    "    open_short_th   =-0.08,\n",
    "    plot=False,\n",
    ")  # The _ is because we don't need return the portfolio object we are just looking for the output\n",
    "alt_param_pf.plot().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2f4f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_feature_importance(final_pipeline, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a93f063",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Combine the test predictions and labels into a single array\n",
    "test_data = np.column_stack((test_preds, test_labels))\n",
    "n_clusters = 12\n",
    "# Run KMeans clustering with 6 clusters\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "\n",
    "# Create a pipeline with all the data\n",
    "pipeline_all = create_pipeline(X, model='xgb')\n",
    "\n",
    "# Evaluate the pipeline with cross-validation\n",
    "scores_all = cross_val_score(pipeline_all, X, y, cv=50, scoring='neg_mean_squared_error')\n",
    "print(f'All data scores: {scores_all}')\n",
    "\n",
    "# Evaluate the pipeline with cross-validation using the clustering step\n",
    "scores_filtered = []\n",
    "for train_index, test_index in KFold(n_splits=50).split(X):\n",
    "    print(f'Fold {len(scores_filtered) + 1}')\n",
    "    # Fit the clustering algorithm on the training data\n",
    "    kmeans.fit(np.column_stack((y[train_index], pipeline_all.predict(X[train_index]).reshape(-1, 1))))\n",
    "\n",
    "    # Get the cluster labels for the test data\n",
    "    cluster_labels = kmeans.predict(np.column_stack((y[test_index], pipeline_all.predict(X[test_index]).reshape(-1, 1))))\n",
    "\n",
    "    # Filter out the data points in the first cluster\n",
    "    filtered_data = X[test_index][cluster_labels != 0]\n",
    "    filtered_labels = y[test_index][cluster_labels != 0]\n",
    "\n",
    "    # Evaluate the pipeline on the filtered data\n",
    "    score = pipeline_all.score(filtered_data, filtered_labels)\n",
    "    scores_filtered.append(score)\n",
    "\n",
    "print(f'Filtered data scores: {scores_filtered}')\n",
    "\n",
    "# Plot the original test data\n",
    "plt.scatter(test_labels, test_preds, alpha=0.5)\n",
    "\n",
    "# Loop through the unique cluster labels and plot the data points for each cluster with a different color\n",
    "for label in np.unique(cluster_labels):\n",
    "    filtered_data = test_data[cluster_labels == label]\n",
    "    plt.scatter(filtered_data[:, 1], filtered_data[:, 0], alpha=0.5, label=f'Cluster {label}')\n",
    "\n",
    "# Add a legend, axis labels, and a title\n",
    "plt.legend()\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Test Predictions vs Actuals')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9df7cc0b",
   "metadata": {},
   "source": [
    "Save the model trained up to 2023 on \"in sample\" data using cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84262055",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename = 'models/model_upto_2023_rolling.joblib'\n",
    "dump(final_pipeline, filename)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec2b6a55",
   "metadata": {},
   "source": [
    "### Load the model from storage and train on out of sample data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9436f31c",
   "metadata": {},
   "source": [
    "### Walk Forward Cross Validation on Out of sample data\n",
    "- Load the model\n",
    "- Preprocess the data\n",
    "- Create Cross Validations for training and testing on newly seen data\n",
    "- Train the model\n",
    "- Make predictions\n",
    "- Test and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4d728a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = 'models/model_upto_2023_rolling.joblib'\n",
    "# # Load the model from the .joblib file\n",
    "# final_pipeline = load(filename) \n",
    "\n",
    "# Create your cross validation splits\n",
    "cv_splitter_oos, cv_oos = create_rolling_cv_with_gap(Xoos, length=windowsize, split=split, gap=periods_future, set_labels=[\"train\", \"test\"])\n",
    "\n",
    "# Train and cross-validate\n",
    "final_pipeline, oos_test_labels, oos_test_preds = cross_validate_and_train(final_pipeline, Xoos, yoos, cv_splitter_oos, model_name=\"Out-Of-Sample\", verbose_interval=10)\n",
    "\n",
    "# Evaluate\n",
    "mse, rmse, mae, r2 = evaluate_predictions(oos_test_labels, oos_test_preds, model_name=\"Out-Of-Sample\")\n",
    "\n",
    "# plot the scatterplot\n",
    "plot_prediction_vs_actual(oos_test_preds, oos_test_labels, title='Out-Of-Sample Predictions vs Actual Results')\n",
    "extract_feature_importance(final_pipeline, X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078913c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "oos_retraining_pf = vbt.Portfolio.from_signals(\n",
    "    outofsample_data.close[oos_test_preds.index], # use only the test set\n",
    "    entries         = oos_test_preds > 0.005, # long entry when prediction is greater than X%\n",
    "    exits           = oos_test_preds < 0.00, # exit long when prediction is negative\n",
    "    short_entries   = oos_test_preds < -0.005, # enter short when prediction is less than -X%\n",
    "    short_exits     = oos_test_preds > 0.0, # exit short when prediction is positive\n",
    "    # direction=\"both\" # long and short\n",
    ")\n",
    "print(oos_retraining_pf.stats())\n",
    "oos_retraining_pf.plot().show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1cf428",
   "metadata": {},
   "outputs": [],
   "source": [
    "oos_retraining_pf.trades.records_readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6014dbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "total_filename = 'models/out_of_sample_rolling.joblib'\n",
    "dump(final_pipeline, total_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427cb20e",
   "metadata": {},
   "source": [
    "### Simulate a portfolio in 2023 with retraining the model every 200 bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad0fd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "oos_test_preds.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3beeb0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = outofsample_data.close[oos_test_preds.index].vbt.plot()\n",
    "outofsample_data.close.vbt.plot(fig=fig).show()\n",
    "data.close.vbt.overlay_with_heatmap(test_preds).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2dce4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insample_pf.orders.records_readable   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04cfc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insample_pf.trades.records_readable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a487d1",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed1f2a44",
   "metadata": {},
   "source": [
    "# Combine insample with out of sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82c4832",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = insample_pf.cumulative_returns.vbt.plot(trace_kwargs=dict(name='Insample')) # plot the in sample equity curve from test data not trained data\n",
    "oos = insample_pf.cumulative_returns[-1] *(1+ oos_retraining_pf.returns).cumprod() # append the out of sample equity curve to the in sample equity curve\n",
    "# Add the out of sample equity curve to the plot\n",
    "oos.vbt.plot(fig=fig, trace_kwargs=dict(name='Out of Sample'))\n",
    "normalized_price = data.close/data.close[0]\n",
    "oos_normalized_price = outofsample_data.close/outofsample_data.close[0] * normalized_price[-1] # normalize the out of sample data to the last price of the in sample data\n",
    "normalized_price.rename('Normalized Price').vbt.plot(fig=fig)\n",
    "oos_normalized_price.rename('Out of Sample Normalized Price').vbt.plot(fig=fig)\n",
    "# The gap is the warmup period for the new model to start making predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a164b3e2",
   "metadata": {},
   "source": [
    "## Save everything to the models folder for later analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81866352",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_vs_actuals = pd.concat([test_preds, test_labels], axis=1)\n",
    "test_preds_vs_actuals.columns = ['Predictions', 'Actuals']\n",
    "oos_test_preds_vs_actuals = pd.concat([oos_test_preds, oos_test_labels], axis=1)\n",
    "oos_test_preds_vs_actuals.columns = ['Predictions', 'Actuals']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9a72c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "oos_test_preds_vs_actuals.tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8106222f",
   "metadata": {},
   "outputs": [],
   "source": [
    "insample_pf.save('models/insample_test_portfolio_rolling.pkl')\n",
    "insample_pf.stats().to_csv('models/insample_stats_test_rolling.csv')\n",
    "insample_pf.trades.records_readable.to_csv('models/insample_trades_test_rolling.csv')\n",
    "X.to_csv('models/insample_X_test_rolling.csv')\n",
    "y.to_csv('models/insample_y_test_rolling.csv')\n",
    "Xoos.to_csv('models/oos_X_test_rolling.csv')\n",
    "yoos.to_csv('models/oos_y_test_rolling.csv')\n",
    "test_preds_vs_actuals.to_csv('models/insample_preds_vs_actuals_test_rolling.csv')\n",
    "oos_test_preds_vs_actuals.to_csv('models/oos_preds_vs_actuals_test_rolling.csv')\n",
    "oos_retraining_pf.save('models/oos_retrained_portfolio_rolling.pkl')\n",
    "oos_retraining_pf.stats().to_csv('models/oos_retrained_stats_rolling.csv')\n",
    "oos_retraining_pf.trades.records_readable.to_csv('models/oos_retrained_trades_rolling.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8e522850",
   "metadata": {},
   "source": [
    "# Explore which features are impacting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7460cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "extract_feature_importance(final_pipeline, X)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb8c76a3",
   "metadata": {},
   "source": [
    "A lot to unpack up above. Why are the feature scores so much different than the fscores of the features?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f1a93d2",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f429f2b",
   "metadata": {},
   "source": [
    "### Randomized Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb34aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Specify hyperparameters to tune and their respective distributions\n",
    "param_dist = {\n",
    "    'model__learning_rate': uniform(0.01, 0.2),\n",
    "    'model__n_estimators': randint(100, 1000),\n",
    "    'model__max_depth': randint(3, 10),\n",
    "    'model__min_child_weight': randint(1, 10),\n",
    "    'model__subsample': uniform(0.5, 0.5),\n",
    "    'model__colsample_bytree': uniform(0.5, 0.5),\n",
    "    # add other parameters here\n",
    "}\n",
    "\n",
    "# Perform randomized search\n",
    "random_search = RandomizedSearchCV(pipeline, param_dist, n_iter=10, cv=cv, scoring=\"neg_mean_squared_error\", n_jobs=-1, verbose=10, random_state=42)\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# Best parameters and score from random search\n",
    "print(f\"Best parameters: {random_search.best_params_}\")\n",
    "print(f\"Best score: {random_search.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4368877a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_splitter, cv = create_rolling_cv_with_gap(X, length=windowsize, split=split, gap=gap, set_labels=['train', 'test'])\n",
    "\n",
    "# The slices are obtained by using your cv_splitter on your X and y.\n",
    "X_slices = cv_splitter.take(X)\n",
    "y_slices = cv_splitter.take(y)\n",
    "\n",
    "# Fit and predict with the best estimator\n",
    "test_labels = []\n",
    "test_preds = []\n",
    "for split in X_slices.index.unique(level=\"split\"):  \n",
    "    X_train_slice = X_slices[(split, \"train\")]  \n",
    "    y_train_slice = y_slices[(split, \"train\")]\n",
    "    X_test_slice = X_slices[(split, \"test\")]\n",
    "    y_test_slice = y_slices[(split, \"test\")]\n",
    "\n",
    "    slice_pipeline = random_search.best_estimator_.fit(X_train_slice, y_train_slice)  # uses the best estimator from the random search\n",
    "    test_pred = slice_pipeline.predict(X_test_slice)  \n",
    "    test_pred = pd.Series(test_pred, index=y_test_slice.index)\n",
    "    test_labels.append(y_test_slice)\n",
    "    test_preds.append(test_pred)\n",
    "    print(f\"MSE for split {split}: {mean_squared_error(y_test_slice, test_pred)}\")\n",
    "\n",
    "\n",
    "test_labels = pd.concat(test_labels).rename(\"labels\")  \n",
    "test_preds = pd.concat(test_preds).rename(\"preds\")\n",
    "\n",
    "# Show the accuracy of the predictions\n",
    "# Assuming test_labels and test_preds are your true and predicted values\n",
    "mse = mean_squared_error(test_labels, test_preds)\n",
    "rmse = np.sqrt(mse)  # or use mean_squared_error with squared=False\n",
    "mae = mean_absolute_error(test_labels, test_preds)\n",
    "r2 = r2_score(test_labels, test_preds)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "\n",
    "# Visualize the predictions as a heatmap plotted against the price\n",
    "# data.close.vbt.overlay_with_heatmap(test_preds).show_svg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba88ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model with the best parameters\n",
    "import json\n",
    "\n",
    "# Save the model with the best parameters\n",
    "dump(random_search.best_estimator_, 'models/xgboost_best_estimator_rolling.joblib')\n",
    "\n",
    "# Save best params dictionary \n",
    "with open('models/xgboost_best_params_rolling.json', 'w') as fp:\n",
    "    json.dump(random_search.best_params_, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f07697",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hyperopt_pf = vbt.Portfolio.from_signals(\n",
    "    data.close[test_preds.index],  # use only the test set\n",
    "    entries         = test_preds > 0.04,  # long when probability of price increase is greater than 2%\n",
    "    exits           = test_preds < 0.00,  # long when probability of price increase is greater than 2%\n",
    "    short_entries   = test_preds < -0.04,  # long when probability of price increase is greater than 2%\n",
    "    short_exits     = test_preds > 0.0,  # short when probability prediction is less than -5%\n",
    "    # direction=\"both\" # long and short\n",
    ")\n",
    "print(hyperopt_pf.stats())\n",
    "hyperopt_pf.plot().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e6d01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperopt_pf.save('models/hyperopt_portfolio_rolling.pkl')\n",
    "hyperopt_pf.trades.records_readable.to_csv('models/hyperopt_trades_rolling.csv')\n",
    "hyperopt_pf.orders.records_readable.to_csv('models/hyperopt_orders_rolling.csv')\n",
    "test_preds.to_csv('models/hyperopt_preds_rolling.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6fd079e2",
   "metadata": {},
   "source": [
    "### Grid Search Method\n",
    "#### DONT RUN WITHOUT GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c7709d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # Specify hyperparameters to tune and their respective ranges\n",
    "# param_grid = {\n",
    "#     'model__learning_rate': [0.01, 0.1, 0.2],\n",
    "#     'model__n_estimators': [100, 500, 1000],\n",
    "#     'model__max_depth': [3, 5, 7],\n",
    "#     'model__min_child_weight': [1, 5, 10],\n",
    "#     'model__subsample': [0.5, 0.7, 1.0],\n",
    "#     'model__colsample_bytree': [0.5, 0.7, 1.0]\n",
    "#     # add other parameters here\n",
    "# }\n",
    "\n",
    "# # Perform grid search\n",
    "# grid_search = GridSearchCV(pipeline, param_grid, cv=cv, scoring=\"r2\", n_jobs=-1, verbose=10)\n",
    "# grid_search.fit(X, y)\n",
    "\n",
    "# # Best parameters and score from grid search\n",
    "# print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "# print(f\"Best score: {grid_search.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d3f992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_splitter, cv = create_rolling_cv(X, length=200, split=.9)\n",
    "\n",
    "# # The slices are obtained by using your cv_splitter on your X and y.\n",
    "# X_slices = cv_splitter.take(X)\n",
    "# y_slices = cv_splitter.take(y)\n",
    "\n",
    "# # Here, we train the model using the slices and the best estimator from your RandomizedSearchCV\n",
    "# test_labels, test_preds, final_pipeline = cross_validate_and_train(random_search.best_estimator_, X_slices, y_slices, cv_splitter, model_name=\"Random Search Best Estimator\")\n",
    "\n",
    "# # And now we evaluate the predictions.\n",
    "# mse, rmse, mae, r2 = evaluate_predictions(test_labels, test_preds, model_name=\"Random Search Best Estimator\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58082fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_pf = vbt.Portfolio.from_signals(\n",
    "#     data.close[test_preds.index], # use only the test set\n",
    "#     entries         = test_preds > 0.05, # long when prediction > X%\n",
    "#     exits           = test_preds < 0.00, # exit when prediction is negative\n",
    "#     short_entries   = test_preds < -0.05, # short when prediction < -X%\n",
    "#     short_exits     = test_preds > 0.00, # exit when prediction is positive\n",
    "# )\n",
    "# print(grid_pf.stats())\n",
    "# grid_pf.plot().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fdcb48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
