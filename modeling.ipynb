{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c69786c2-7c5b-4b1e-bd0c-52380c8df261",
   "metadata": {},
   "source": [
    "# Cross-validation\n",
    "Once we develop a rule-based or ML-based strategy, it's time to backtest it. The first time around we obtain a low Sharpe ratio we're unhappy with, we decide to tweak our strategy. Eventually, after multiple iterations of tweaking parameters, we end up with a \"flawless\" combination of parameters and a strategy with an exceptional Sharpe ratio. However, in live trading the performance took a different turn: we essentially tanked and lost money. What went wrong?\n",
    "\n",
    "Markets inherently have noise - small and frequent idiosyncrasies in the price data. When modelling a strategy, we want to avoid optimizing for one specific period because there is a chance the model adapts so closely to historical data that it becomes ineffective in predicting the future. It'd be like tuning a car specifically for one racetrack, while expecting it to perform well everywhere. Especially with vectorbt, which enables us to search extensive databases of historical market data for patterns, it is often possible to develop elaborate rules that appear to predict price development with close accuracy (see p-hacking) but make random guesses when applied to data outside the sample the model was constructed from.\n",
    "\n",
    "Overfitting (aka curve fitting) usually occurs for one or more of the following reasons: mistaking noise for signal, and overly tweaking too many parameters. To curb overfitting, we should use cross-validation (CV), which involves partitioning a sample of data into complementary subsets, performing the analysis on one subset of data called the training or in-sample (IS) set, and validating the analysis on the other subset of data called the validation or out-of-sample (OOS) set. This procedure is repeated until we have multiple OOS periods and can draw statistics from these results combined. The ultimate questions we need to ask ourselves: is our choice of parameters robust in the IS periods? Is our performance robust on the OOS periods? Because if not, we're shooting in the dark, and as a quant investor we should not leave room for second-guessing when real money is at stake.\n",
    "\n",
    "Consider a simple strategy around a moving average crossover.\n",
    "\n",
    "First, we'll pull some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2751478-95ce-47d2-9e80-4d47ed0c7b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numba import njit\n",
    "\n",
    "import vectorbtpro as vbt\n",
    "vbt.settings.set_theme(\"dark\")\n",
    "vbt.settings.plotting[\"layout\"][\"width\"] = 800\n",
    "vbt.settings.plotting['layout']['height'] = 200\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a78505c",
   "metadata": {},
   "source": [
    "pull in some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b45492f-d152-43e2-88a9-ab95bbffd546",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = vbt.BinanceData.fetch(\"BTCUSDT\", end=\"2022-11-01 UTC\")\n",
    "data.index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e3f9cf8",
   "metadata": {},
   "source": [
    "Let's construct a parameterized mini-pipeline that takes data and the parameters, and returns the Sharpe ratio that should reflect the performance of our strategy on that test period:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "935ca582-ef09-40f9-b6ff-c303c98989b1",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "The class Splitter can also be helpful in cross-validating ML models. In particular, you can casually step upon a class SKLSplitter that acts as a regular cross-validator from scikit-learn by subclassing BaseCrossValidator. We'll demonstrate its usage on a simple classification problem of predicting the best entry and exit timings.\n",
    "\n",
    "Before we start, we need to decide on features and labels that should act as predictor and response variables respectively. Features are usually multi-columnar time-series DataFrames where each row contains multiple data points (one per column) that should predict the same row in labels. Labels are usually a single-columnar time-series Series that should be predicted. Ask yourself the following questions to easily come up with a decision:\n",
    "\n",
    "\"How can the future performance be represented, preferably as a single number? Should it be the price at the next bar, the average price change over the next week, a vector of weights for rebalancing, a boolean containing a signal, or something else?\"\n",
    "\"What kind of data that encompasses the past performance is likely to predict the future performance? Should it be indicators, news sentiment index, past backtesting results, or something else?\"\n",
    "\"Which ML model can handle such a task?\" (remember that most models are limited to just a couple of specific feature and label formats!)\n",
    "For the sake of an example, we'll fit a random forest classifier on all TA-Lib indicators stacked along columns to predict the binary labels generated by the label generator TRENDLB, where 1 means an uptrend and 0 means a downtrend. Sounds like fun ðŸ˜Œ\n",
    "\n",
    "First, run all the TA-Lib indicators on the entire data to get the feature set X:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9cc747-d0b4-470f-a262-7cfc7ad1d62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.run(\"talib\")\n",
    "print(f'The shape is {X.shape}')\n",
    "print(f'The columns are {X.columns}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3305b9f0",
   "metadata": {},
   "source": [
    "We've got 1902 rows (dates) and 174 columns (features).\n",
    "\n",
    "Next, generate the labels y (we'll use the same configuration as previously): For example, let's analyze the performance of a portfolio during different market regimes. First, we'll use the forward-looking label generator TRENDLB to annotate each data point with either 1 (uptrend), 0 (downtrend), or NaN (cannot be classified). Given the volatility of our data, we'll register an uptrend once the price jumps `up_percentage` % from its previous low point, and a downtrend once the price falls by `down_percentage`% from its previous high point: **If we were running this on our minutely dataframe we would have to edit these significantly.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdb5b1c-e4cc-4472-bdb1-4629492306e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "up_percentage = 1.0 # 0.15 = 15%\n",
    "down_percentage = 0.5\n",
    "\n",
    "trendlb = data.run(\"trendlb\", up_percentage, down_percentage, mode=\"binary\")\n",
    "y = trendlb.labels\n",
    "y.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "adc867c4",
   "metadata": {},
   "source": [
    "Both the features and the labels contain NaNs, which we need to carefully take care of. If we remove the rows with at least one NaN, we'll remove all the data. Instead, we'll first remove the columns that consist entirely of NaNs or a single unique value. Also, because X and y should have the same length, we need to do the row-filtering operation on both datasets simultaneously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1e42f0-7f1f-474c-865b-2fabd6dfc907",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.replace([-np.inf, np.inf], np.nan) # replace inf with nan\n",
    "invalid_column_mask = X.isnull().all(axis=0) | (X.nunique() == 1) # drop columns that are all nan or have only one unique value\n",
    "X = X.loc[:, ~invalid_column_mask] # drop invalid columns\n",
    "invalid_row_mask = X.isnull().any(axis=1) | y.isnull() # drop rows that have nan in any column or in y\n",
    "X = X.loc[~invalid_row_mask] # drop invalid rows in X the training set\n",
    "y = y.loc[~invalid_row_mask] # drop invalid rows in the target\n",
    "X.shape, y.shape # check the shape to make sure it is the same length as y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0bb26be3",
   "metadata": {},
   "source": [
    "**Warning** If you worked with ML before, you'll quickly feel the danger coming from the logical operation in the first cell: we're checking for a condition across the entire column, thus potentially catching the look-ahead bias. Even though our operation isn't too dangerous because we remove only the columns that are likely to stay irrelevant in the future, other transformations such as data normalization should always be included in a Pipeline that's executed per split rather than once and globally.\n",
    "\n",
    "We've successfully removed a total of 129 rows and 30 columns.\n",
    "\n",
    "Next, we'll establish our classifier that will learn X to predict y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1fe438-bf27-4fc3-9d9b-5c71a7b046ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d5a815c0",
   "metadata": {},
   "source": [
    "**Question** Why haven't we rescaled, normalized, or reduced the dimensionality of the features? Random forests are very robust modeling techniques and can handle high noise levels as well as a high number of features.\n",
    "\n",
    "To cross-validate the classifier, let's create an SKLSplitter instance that splits the entire period into expanding windows with non-overlapping test periods of 180 bars:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64bbb84-5340-4554-a78c-5a58bdbe5010",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = vbt.SKLSplitter(\n",
    "    \"from_expanding\", \n",
    "    min_length=360, \n",
    "    offset=180, \n",
    "    split=-180,\n",
    "    set_labels=[\"train\", \"test\"]\n",
    ") # use expanding window cross-validation\n",
    "\n",
    "cv_splitter = cv.get_splitter(X)\n",
    "cv_splitter.plot().show_svg()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13d07da7",
   "metadata": {},
   "source": [
    "Finally, run the classifier on each training period and check the accuracy of its predictions on the respective test period. Even though the accuracy score is the most basic of all classification scores and has its own flaws, we'll keep things simplified for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c26f554-c0ff-4973-a625-d95549bbbe36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_score(clf, X, y, cv=cv, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580ae7c7-d108-454f-903f-937c8dcf058e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_slices = cv_splitter.take(X)\n",
    "y_slices = cv_splitter.take(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9186a59-798a-404a-b763-f6360d098dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = []\n",
    "test_preds = []\n",
    "for split in X_slices.index.unique(level=\"split\"):\n",
    "    X_train_slice = X_slices[(split, \"train\")]\n",
    "    y_train_slice = y_slices[(split, \"train\")]\n",
    "    X_test_slice = X_slices[(split, \"test\")]\n",
    "    y_test_slice = y_slices[(split, \"test\")]\n",
    "    slice_clf = clf.fit(X_train_slice, y_train_slice)\n",
    "    test_pred = slice_clf.predict(X_test_slice)\n",
    "    test_pred = pd.Series(test_pred, index=y_test_slice.index)\n",
    "    test_labels.append(y_test_slice)\n",
    "    test_preds.append(test_pred)\n",
    "    \n",
    "test_labels = pd.concat(test_labels).rename(\"labels\")\n",
    "test_preds = pd.concat(test_preds).rename(\"preds\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "250acb41",
   "metadata": {},
   "source": [
    "Let's compare the actual labels to the predictions:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411d961d-7e24-427a-92e1-07c6fbc52f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.close.vbt.overlay_with_heatmap(test_labels).show_svg()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b576f5f0",
   "metadata": {},
   "source": [
    "The model seems to correctly classify many bigger uptrends and even issue an exit signal at the latest peak on time! Nevertheless, we shouldn't just rely on our visual intuition: let's backtest the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a904c9f0-9514-4c17-bc14-a2d9980debe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.close.vbt.overlay_with_heatmap(test_preds).show_svg()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "21640945",
   "metadata": {},
   "source": [
    "Looks pretty good, now let's run a backtest to see if the eyes are deceiving us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0592fce-7a89-4fc6-9206-1ca7b3a51700",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = vbt.Portfolio.from_signals(\n",
    "    data.close[test_preds.index], # use only the test set\n",
    "    test_preds == 1, # long when preds == 1\n",
    "    test_preds == 0, # short when preds == 0\n",
    "    direction=\"both\" # long and short\n",
    ")\n",
    "pf.stats()\n",
    "pf['2021'].plot().show_svg()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ccf045db",
   "metadata": {},
   "source": [
    "We've got some pretty solid statistics ðŸŒŸ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41119985",
   "metadata": {},
   "source": [
    "If you're willing to accept a challenge: build a pipeline to impute and (standard-)normalize the data, [reduce the dimensionality](https://scikit-learn.org/stable/auto_examples/compose/plot_digits_pipe.html) of the features, as well as fit one of the [linear](https://scikit-learn.org/stable/modules/linear_model.html) models to predict the average price change over the next n bars (i.e., regression task!). Based on each prediction, you can then decide whether a position is worth opening or closing out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00364d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68adf328",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression, LogisticRegression\n",
    "from sklearn.svm import SVR, SVC\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "\n",
    "# data is a vbt.BinanceData object with OHLCV data for BTCUSDT\n",
    "\n",
    "# Generate the features (X) using TA-Lib indicators\n",
    "X = data.run(\"talib\")\n",
    "# add trend label as a feature if the market is up 20% it is in a bullish trend and if it is down 5% it is in a bearish trend\n",
    "# Read more about the pivotinfo below in the next couple cells\n",
    "pivot_info = data.run(\"pivotinfo\", up_th=.10, down_th=0.05)\n",
    "binary_pivot_labels = np.where(data.close > pivot_info.conf_value,1,0) # Create binary labels for pivot points\n",
    "X['trend'] = binary_pivot_labels # add pivot label as a feature \n",
    "# Generate the labels (y) using trend label generator with 100% up and 50% down\n",
    "# trendlb = data.run(\"trendlb\", 1.0, 0.5, mode=\"binary\") # Trendlb is a label generator to indicate if we are in an uptrend 1, or downtrend 0\n",
    "# y = trendlb.labels\n",
    "\n",
    "# Now we are trying to generate future price predictions so we will set the y labels to the price change n periods in the future\n",
    "n = 90 # number of periods in the future to predict\n",
    "y = (data.close.shift(-n) / data.close - 1).rolling(n).mean() # future price change we use rolling mean to smooth the data\n",
    "\n",
    "# Preprocessing steps to handle NaNs\n",
    "X = X.replace([-np.inf, np.inf], np.nan) # replace inf with nan\n",
    "invalid_column_mask = X.isnull().all(axis=0) | (X.nunique() == 1) # drop columns that are all nan or have only one unique value\n",
    "X = X.loc[:, ~invalid_column_mask] # drop invalid columns\n",
    "invalid_row_mask = X.isnull().any(axis=1) | y.isnull() # drop rows that have nan in any column or in y\n",
    "\n",
    "# Drop invalid rows in X and y\n",
    "X = X.loc[~invalid_row_mask]\n",
    "y = y.loc[~invalid_row_mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388bd3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Construct the pipeline\n",
    "steps = [\n",
    "    ('imputation', SimpleImputer(strategy='mean')),  # Imputation replaces missing values\n",
    "    ('scaler', StandardScaler()),  # StandardScaler normalizes the data\n",
    "    ('pca', PCA(n_components=15)),  # PCA reduces dimensionality\n",
    "    \n",
    "    # Choose one of the following models\n",
    "    # ('model', Ridge())  # Ridge regression is used as the prediction model\n",
    "    # ('model', LinearRegression())  # Linear regression is used as the prediction model\n",
    "    # ('model', LogisticRegression())  # Logistic regression is used as the prediction model\n",
    "    # ('model', Lasso())  # Lasso regression is used as the prediction model\n",
    "    # ('model', ElasticNet())  # ElasticNet regression is used as the prediction model\n",
    "    # ('model', SVR())  # Support Vector Regression is used as the prediction model\n",
    "    ('model', XGBRegressor(objective='reg:squarederror'))  # XGBoost regression is used as the prediction model\n",
    "]\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Cross-validate\n",
    "cv = vbt.SKLSplitter(\n",
    "    \"from_expanding\",\n",
    "    min_length=360,\n",
    "    offset=10,\n",
    "    split=-10,\n",
    "    set_labels=[\"train\", \"test\"]\n",
    ")\n",
    "\n",
    "cv_splitter = cv.get_splitter(X)\n",
    "cv_splitter.plot().show_svg()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98598249",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X, y)  # Fit the pipeline on the entire dataset    \n",
    "print(f'Pipeline Score :{pipeline.score(X, y)}')  # Score the pipeline on the entire dataset\n",
    "scores = cross_val_score(pipeline, X, y, cv=cv, scoring=\"r2\", n_jobs=-1, verbose=100) # how well the model generalizes to unseen data\n",
    "average_score = np.mean(scores)\n",
    "print(f'Average cross-validation score: {average_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcef73c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_slices = cv_splitter.take(X)\n",
    "y_slices = cv_splitter.take(y)\n",
    "\n",
    "test_labels = []\n",
    "test_preds = []\n",
    "for split in X_slices.index.unique(level=\"split\"):  \n",
    "    X_train_slice = X_slices[(split, \"train\")]  \n",
    "    y_train_slice = y_slices[(split, \"train\")]\n",
    "    X_test_slice = X_slices[(split, \"test\")]\n",
    "    y_test_slice = y_slices[(split, \"test\")]\n",
    "    slice_pipeline = pipeline.fit(X_train_slice, y_train_slice)  \n",
    "    test_pred = slice_pipeline.predict(X_test_slice)  \n",
    "    test_pred = pd.Series(test_pred, index=y_test_slice.index)\n",
    "    test_labels.append(y_test_slice)\n",
    "    test_preds.append(test_pred)\n",
    "\n",
    "test_labels = pd.concat(test_labels).rename(\"labels\")  \n",
    "test_preds = pd.concat(test_preds).rename(\"preds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6f3b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Assuming test_labels and test_preds are your true and predicted values\n",
    "mse = mean_squared_error(test_labels, test_preds)\n",
    "rmse = np.sqrt(mse)  # or use mean_squared_error with squared=False\n",
    "mae = mean_absolute_error(test_labels, test_preds)\n",
    "r2 = r2_score(test_labels, test_preds)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"R-squared: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff151d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.close.vbt.overlay_with_heatmap(test_preds).show_svg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dd5893",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = vbt.Portfolio.from_signals(\n",
    "    data.close[test_preds.index], # use only the test set\n",
    "    entries         = test_preds > 0.25, # long when price prediction is greater than ...%\n",
    "    exits           = test_preds < 0.0, # exit long when price prediction is less than ...%\n",
    "    short_entries   = test_preds < -0.15, # Enter Short when price prediction is ...%\n",
    "    short_exits     = test_preds > 0.00, # Exit short when probability prediction is ...%\n",
    "    # direction=\"both\" # long and short\n",
    ")\n",
    "print(pf.stats())\n",
    "pf.plot().show_svg()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eef4fc9a",
   "metadata": {},
   "source": [
    "Check out a couple of different regimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e624b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf['2018':'2021'].plot().show_svg()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e657c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf['2021':'2023'].plot().show_svg()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2084b4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "946cfa86",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
