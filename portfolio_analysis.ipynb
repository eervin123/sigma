{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import vectorbtpro as vbt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vbt.settings.plotting[\"layout\"][\"template\"] = \"vbt_dark\"\n",
    "vbt.settings.plotting[\"layout\"][\"width\"] = 1200\n",
    "vbt.settings.plotting['layout']['height'] = 200\n",
    "vbt.settings.wrapping[\"freq\"] = \"1m\"\n",
    "# vbt.settings.portfolio['size_granularity'] = 0.001\n",
    "vbt.settings.portfolio['init_cash'] = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/Users/ericervin/Documents/Coding/data-repository/data/BTCUSDT_1m_futures.pkl' \n",
    "\n",
    "futures_1m = vbt.BinanceData.load(data_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dollar Bar Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def dollar_bar_func(ohlc_df, dollar_bar_size):\n",
    "    # Calculate dollar value traded for each row\n",
    "    ohlc_df['DollarValue'] = ohlc_df['Close'] * ohlc_df['Volume']\n",
    "    \n",
    "    # Calculate cumulative dollar value\n",
    "    ohlc_df['CumulativeDollarValue'] = ohlc_df['DollarValue'].cumsum()\n",
    "    \n",
    "    # Determine the number of dollar bars\n",
    "    num_bars = int(ohlc_df['CumulativeDollarValue'].iloc[-1] / dollar_bar_size)\n",
    "    \n",
    "    # Generate index positions for dollar bars\n",
    "    bar_indices = [0]\n",
    "    cumulative_value = 0\n",
    "    for i in range(1, len(ohlc_df)):\n",
    "        cumulative_value += ohlc_df['DollarValue'].iloc[i]\n",
    "        if cumulative_value >= dollar_bar_size:\n",
    "            bar_indices.append(i)\n",
    "            cumulative_value = 0\n",
    "    \n",
    "    # Create a new dataframe with dollar bars\n",
    "    dollar_bars = []\n",
    "    for i in range(len(bar_indices) - 1):\n",
    "        start_idx = bar_indices[i]\n",
    "        end_idx = bar_indices[i + 1]\n",
    "        \n",
    "        dollar_bar = {\n",
    "            'Open': ohlc_df['Open'].iloc[start_idx],\n",
    "            'High': ohlc_df['High'].iloc[start_idx:end_idx].max(),\n",
    "            'Low': ohlc_df['Low'].iloc[start_idx:end_idx].min(),\n",
    "            'Close': ohlc_df['Close'].iloc[end_idx],\n",
    "            'Volume': ohlc_df['Volume'].iloc[start_idx:end_idx].sum(),\n",
    "            'Quote volume': ohlc_df['Quote volume'].iloc[start_idx:end_idx].sum(),\n",
    "            'Trade count': ohlc_df['Trade count'].iloc[start_idx:end_idx].sum(),\n",
    "            'Taker base volume': ohlc_df['Taker base volume'].iloc[start_idx:end_idx].sum(),\n",
    "            'Taker quote volume': ohlc_df['Taker quote volume'].iloc[start_idx:end_idx].sum()\n",
    "        }\n",
    "        \n",
    "        if isinstance(ohlc_df.index, pd.DatetimeIndex):\n",
    "            dollar_bar['Open Time'] = ohlc_df.index[start_idx]\n",
    "            dollar_bar['Close Time'] = ohlc_df.index[end_idx] - pd.Timedelta(milliseconds=1)\n",
    "        elif 'Open Time' in ohlc_df.columns:\n",
    "            dollar_bar['Open Time'] = ohlc_df['Open Time'].iloc[start_idx]\n",
    "            dollar_bar['Close Time'] = ohlc_df['Open Time'].iloc[end_idx] - pd.Timedelta(milliseconds=1)\n",
    "        \n",
    "        dollar_bars.append(dollar_bar)\n",
    "    \n",
    "    dollar_bars_df = pd.concat([pd.DataFrame([bar]) for bar in dollar_bars], ignore_index=True)\n",
    "    \n",
    "    return dollar_bars_df\n",
    "\n",
    "# Create a simple function to simplify the number so we can use it in our column names\n",
    "def simplify_number(num):\n",
    "    \"\"\"\n",
    "    Simplifies a large number by converting it to a shorter representation with a suffix (K, M, B).\n",
    "    simplify_number(1000) -> 1K\n",
    "    \"\"\"\n",
    "    suffixes = ['', 'K', 'M', 'B']\n",
    "    suffix_index = 0\n",
    "\n",
    "    while abs(num) >= 1000 and suffix_index < len(suffixes) - 1:\n",
    "        num /= 1000.0\n",
    "        suffix_index += 1\n",
    "\n",
    "    suffix = suffixes[suffix_index] if suffix_index > 0 else ''\n",
    "    simplified_num = f'{int(num)}{suffix}'\n",
    "\n",
    "    return simplified_num\n",
    "\n",
    "def merge_and_fill_dollar_bars(original_df, dollar_bars_df, dollar_bar_size):\n",
    "    # Add prefix to column names in dollar bars dataframe\n",
    "    dollar_bar_prefix = f'db_{simplify_number(dollar_bar_size)}_'\n",
    "    dollar_bars_df_renamed = dollar_bars_df.add_prefix(dollar_bar_prefix)\n",
    "\n",
    "    # Convert 'Open Time' columns to pandas datetime format and set them as index\n",
    "    dollar_bars_df_renamed.index = pd.to_datetime(dollar_bars_df_renamed[dollar_bar_prefix + 'Open Time'])\n",
    "\n",
    "    # Merge the dataframes on the index\n",
    "    merged_df = original_df.merge(dollar_bars_df_renamed, how='left', left_index=True, right_index=True)\n",
    "\n",
    "    # Set the flag for a new dollar bar with prefix\n",
    "    merged_df[dollar_bar_prefix + 'NewDBFlag'] = ~merged_df[dollar_bar_prefix + 'Close'].isna()\n",
    "\n",
    "    # Forward fill the NaN values for all columns except the new dollar bar flag\n",
    "    columns_to_ffill = [col for col in merged_df.columns if col != dollar_bar_prefix + 'NewDBFlag']\n",
    "    merged_df[columns_to_ffill] = merged_df[columns_to_ffill].fillna(method='ffill')\n",
    "\n",
    "    # Fill the remaining NaN values in the new dollar bar flag column with False\n",
    "    merged_df[dollar_bar_prefix + 'NewDBFlag'] = merged_df[dollar_bar_prefix + 'NewDBFlag'].fillna(False)\n",
    "    \n",
    "    # Assign the renamed 'Open Time' column back to the dataframe\n",
    "    merged_df[dollar_bar_prefix + 'Open Time'] = merged_df[dollar_bar_prefix + 'Open Time']\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dollar Bars from pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rather than make new dollar bars just read in the pkl file\n",
    "dollar_bar_size = 90_000_000\n",
    "btc_90M_db_vbt = vbt.BinanceData.load('data/btc_90M_db_vbt.pkl')\n",
    "btc_90M_copy = btc_90M_db_vbt.get().copy()\n",
    "futures_1m_copy = futures_1m.get().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = btc_90M_db_vbt['2021-01-01':'2023-01-01']\n",
    "outofsample_data = btc_90M_db_vbt['2023-01-01':'2023-06-03']\n",
    "print(data.shape)\n",
    "print(outofsample_data.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the Predictions from the Model\n",
    "Read in the predictions and attach them to the dollar bar dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load, dump\n",
    "insample_model = load('models/model_upto_2023.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = 150 # number of periods in the future to predict\n",
    "\n",
    "# Generate the features (X) using TA-Lib indicators\n",
    "# X = data.run(\"talib\", periods=vbt.run_func_dict(mavp=n))\n",
    "X = data.get().copy()\n",
    "# psar_vbt = data.run(\"pandas_ta:PSAR\", append=True, acceleration=0.02, maximum=0.2)\n",
    "# add trend label as a feature\n",
    "X['trend'] = data.run(\"trendlb\", .2, 0.05, mode=\"binary\").labels # add trend label as a feature\n",
    "# X['psar_cross'] = psar_vbt.psarr\n",
    "\n",
    "# Add time features\n",
    "X['dayofmonth'] = X.index.day\n",
    "X['month'] = X.index.month\n",
    "X['year'] = X.index.year\n",
    "X['hour'] = X.index.hour\n",
    "X['minute'] = X.index.minute\n",
    "X['dayofweek'] = X.index.dayofweek\n",
    "X['dayofyear'] = X.index.dayofyear\n",
    "\n",
    "# Now we are trying to generate future price predictions so we will set the y labels to the price change n periods in the future\n",
    "y = (data.close.shift(-n) / data.close - 1).rolling(n).mean() # future price change we use rolling mean to smooth the data\n",
    "\n",
    "# Preprocessing steps to handle NaNs\n",
    "X = X.replace([-np.inf, np.inf], np.nan) # replace inf with nan\n",
    "invalid_column_mask = X.isnull().all(axis=0) | (X.nunique() == 1) # drop columns that are all nan or have only one unique value\n",
    "X = X.loc[:, ~invalid_column_mask] # drop invalid columns\n",
    "invalid_row_mask = X.isnull().any(axis=1) | y.isnull() # drop rows that have nan in any column or in y\n",
    "\n",
    "# Drop invalid rows in X and y\n",
    "X = X.loc[~invalid_row_mask]\n",
    "y = y.loc[~invalid_row_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Open time and close time\n",
    "X = X.drop(['Open Time','Close Time'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['Close'].vbt.overlay_with_heatmap(X['trend']).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'models/model_upto_2023.joblib'\n",
    "# Load the model from the .joblib file\n",
    "final_pipeline = load(filename) \n",
    "\n",
    "# Make predictions on the entire dataset\n",
    "insample_predictions = final_pipeline.predict(X)\n",
    "\n",
    "# Calculate the R-squared score on the entire dataset\n",
    "r2 = r2_score(y, insample_predictions)\n",
    "\n",
    "print(f\"R-squared on the entire dataset: {r2}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the predictions to this dollar bar dataframe before we merge it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = 150 # number of periods in the future to predict\n",
    "\n",
    "# Generate the features (X) using TA-Lib indicators\n",
    "# X = data.run(\"talib\", periods=vbt.run_func_dict(mavp=n))\n",
    "Xoos = outofsample_data.get()\n",
    "psar_vbt = outofsample_data.run(\"pandas_ta:PSAR\", append=True, acceleration=0.02, maximum=0.2) # I didn't end up using this\n",
    "# add trend label as a feature\n",
    "Xoos['trend'] = outofsample_data.run(\"trendlb\", .2, 0.05, mode=\"binary\").labels # add trend label as a feature\n",
    "# X['psar_cross'] = psar_vbt.psarr\n",
    "# Drop the time columns\n",
    "# Drop Open time and close time\n",
    "Xoos = Xoos.drop(['Open Time','Close Time'], axis=1)\n",
    "# Add time features\n",
    "Xoos['dayofmonth']  = Xoos.index.day\n",
    "Xoos['month']       = Xoos.index.month\n",
    "Xoos['year']        = Xoos.index.year\n",
    "Xoos['hour']        = Xoos.index.hour\n",
    "Xoos['minute']      = Xoos.index.minute\n",
    "Xoos['dayofweek']   = Xoos.index.dayofweek\n",
    "Xoos['dayofyear']   = Xoos.index.dayofyear\n",
    "\n",
    "# Now we are trying to generate future price predictions so we will set the y labels to the price change n periods in the future\n",
    "yoos = (outofsample_data.close.shift(-n) / outofsample_data.close - 1).rolling(n).mean() # future price change we use rolling mean to smooth the data\n",
    "\n",
    "# Preprocessing steps to handle NaNs\n",
    "Xoos = Xoos.replace([-np.inf, np.inf], np.nan) # replace inf with nan\n",
    "invalid_column_mask = Xoos.isnull().all(axis=0) #| (Xoos.nunique() == 1) # removed the second condition because `year` column is always the same for 2023\n",
    "Xoos = Xoos.loc[:, ~invalid_column_mask] # drop invalid columns\n",
    "invalid_row_mask = Xoos.isnull().any(axis=1) | yoos.isnull() # drop rows that have nan in any column or in y\n",
    "\n",
    "# Drop invalid rows in X and y\n",
    "Xoos = Xoos.loc[~invalid_row_mask]\n",
    "yoos = yoos.loc[~invalid_row_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the entire dataset\n",
    "outofsample_predictions = final_pipeline.predict(Xoos)\n",
    "\n",
    "# Calculate the R-squared score on the entire dataset\n",
    "r2 = r2_score(yoos, outofsample_predictions)\n",
    "\n",
    "print(f\"R-squared on the out of sample dataset: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pandas dataframe or series with the predictions and the index from the out of sample data\n",
    "outofsample_predictions = pd.Series(outofsample_predictions, index=yoos.index)\n",
    "outofsample_predictions = outofsample_predictions.rename(\"outofsample_predictions\")\n",
    "print(outofsample_predictions.shape)\n",
    "\n",
    "# create a pandas dataframe or series with the predictions and the index from the out of sample data\n",
    "insample_predictions = pd.Series(insample_predictions, index=X.index)\n",
    "insample_predictions = insample_predictions.rename(\"insample_predictions\")\n",
    "print(insample_predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oos_pf = vbt.Portfolio.from_signals(\n",
    "    outofsample_data.close[outofsample_predictions.index], # use only the test set\n",
    "    entries         = outofsample_predictions > 0.05, # long when probability of price increase is greater than 2%\n",
    "    exits           = outofsample_predictions < 0.00, # long when probability of price increase is greater than 2%\n",
    "    short_entries   = outofsample_predictions < -0.04, # long when probability of price increase is greater than 2%\n",
    "    short_exits     = outofsample_predictions > 0.0, # short when probability prediction is less than -5%\n",
    "    # direction=\"both\" # long and short\n",
    ")\n",
    "print(oos_pf.stats())\n",
    "oos_pf.plot().show_svg()\n",
    "# For comparison run a buy and hold strategy\n",
    "buy_and_hold = vbt.Portfolio.from_holding(outofsample_data.close[outofsample_predictions.index])\n",
    "print(f'Total return: {buy_and_hold.total_return}')\n",
    "print(f'Max Drawdown: {buy_and_hold.max_drawdown}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insample_pf = vbt.Portfolio.from_signals(\n",
    "    data.close[insample_predictions.index],  # use only the test set\n",
    "    entries         = insample_predictions > 0.05,  # long when probability of price increase is greater than 2%\n",
    "    exits           = insample_predictions < 0.00,  # long when probability of price increase is greater than 2%\n",
    "    short_entries   = insample_predictions < -0.04,  # long when probability of price increase is greater than 2%\n",
    "    short_exits     = insample_predictions > 0.0,  # short when probability prediction is less than -5%\n",
    "    # direction=\"both\" # long and short\n",
    ")\n",
    "print(insample_pf.stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insample_pf.resample('1d').plot().show_svg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe with the dollar bars merged into the original dataframe\n",
    "full_df = merge_and_fill_dollar_bars(futures_1m_copy, btc_90M_copy, dollar_bar_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
